import json
import uuid
from typing import List, Dict, Any, AsyncGenerator
import openai
from anthropic import Anthropic
import asyncio

from aletheia_backend.core.config import settings


class VerdictAgent:
    """
    é‰´å®šç»“è®º Agent - å¤šç»´åº¦æ·±åº¦åˆ†æä¸“å®¶
    
    èŒè´£ï¼š
    1. ä»å¤šè§’åº¦å®¡è§†é—®é¢˜ï¼Œä¸å±€é™äºå­—é¢æ„æ€
    2. æ·±åº¦åˆ†æå…³é”®ä¿¡æºå’Œæ™®é€šä¿¡æº
    3. è¯†åˆ«é—®é¢˜çš„å¤šä¸ªå±‚é¢ï¼ˆäº‹å®ã€èƒŒæ™¯ã€åŠ¨æœºã€å½±å“ï¼‰
    4. ç»¼åˆ Search Agent çš„åˆ†æç»“æœ
    5. å¾—å‡ºæœ‰è¯´æœåŠ›ã€ä»¤äººä¿¡æœçš„ç»“è®º
    """

    def __init__(self):
        self.llm_provider = settings.LLM_PROVIDER
        model = settings.VERDICT_LLM_MODEL or settings.OPENAI_MODEL
        self.openai_client = openai.AsyncOpenAI(
            api_key=settings.OPENAI_API_KEY,
            base_url=settings.OPENAI_BASE_URL,
            timeout=60.0
        ) if settings.OPENAI_API_KEY else None
        self.anthropic_client = Anthropic(
            api_key=settings.ANTHROPIC_API_KEY
        ) if settings.ANTHROPIC_API_KEY else None
        self.model = model
        self.temperature = settings.VERDICT_LLM_TEMPERATURE

    async def verdict(self, search_result: Dict[str, Any], original_content: str) -> Dict[str, Any]:
        """
        åŸºäº Search Agent çš„æ·±åº¦åˆ†æç»“æœï¼Œç”Ÿæˆå¤šç»´åº¦é‰´å®šç»“è®º
        """
        print(f"[VerdictAgent] Starting multi-dimensional verdict")
        verdict_id = str(uuid.uuid4())

        # æå– Search Agent çš„åˆ†æç»“æœ
        key_sources = search_result.get("key_sources", [])
        regular_sources = search_result.get("regular_sources", [])
        all_sources = search_result.get("all_sources", [])
        search_analysis = search_result.get("analysis", {})
        query_analysis = search_result.get("query_analysis", {})

        if not all_sources:
            print("[VerdictAgent] No sources found, returning unverifiable result")
            return self._create_unverifiable_result(verdict_id, search_result.get("search_id"))

        # é˜¶æ®µ1: å¤šç»´åº¦é—®é¢˜åˆ†è§£
        print("[VerdictAgent] Performing multi-dimensional analysis...")
        dimensions = await self._analyze_dimensions(
            original_content, query_analysis, search_analysis
        )

        # é˜¶æ®µ2: æ·±åº¦è¯æ®è¯„ä¼°
        print("[VerdictAgent] Evaluating evidence...")
        evidence_evaluation = await self._evaluate_evidence_comprehensive(
            key_sources, regular_sources, search_analysis, original_content
        )

        # é˜¶æ®µ3: å¤šè§’åº¦ç»¼åˆåˆ¤æ–­
        print("[VerdictAgent] Synthesizing multi-angle judgment...")
        final_judgment = await self._synthesize_judgment(
            original_content, dimensions, evidence_evaluation, search_analysis
        )

        # æ„å»ºè¯æ®é“¾
        evidence_chain = self._build_comprehensive_evidence_chain(
            key_sources, regular_sources, final_judgment.get("supporting_sources", [])
        )

        print(f"[VerdictAgent] Verdict complete: {final_judgment.get('conclusion')} with confidence {final_judgment.get('confidence_score')}")

        return {
            "verdict_id": verdict_id,
            "search_task_ref": search_result.get("search_id"),
            
            # é‰´å®šç»“è®º
            "conclusion": final_judgment.get("conclusion", "uncertain"),
            "confidence_score": final_judgment.get("confidence_score", 0.5),
            "conclusion_summary": final_judgment.get("summary", ""),
            
            # å¤šç»´åº¦åˆ†æ
            "dimensional_analysis": {
                "factual_dimension": dimensions.get("factual", {}),
                "contextual_dimension": dimensions.get("contextual", {}),
                "motivational_dimension": dimensions.get("motivational", {}),
                "impact_dimension": dimensions.get("impact", {})
            },
            
            # è¯¦ç»†æ¨ç†é“¾
            "reasoning_chain": final_judgment.get("reasoning_chain", []),
            "multi_angle_reasoning": final_judgment.get("multi_angle_reasoning", {}),
            
            # è¯æ®è¯„ä¼°
            "evidence_evaluation": evidence_evaluation,
            
            # è¯æ®é“¾
            "evidence_chain": evidence_chain,
            
            # å‘ç°åˆ†ç±»
            "findings": {
                "verified_claims": final_judgment.get("verified_claims", []),
                "refuted_claims": final_judgment.get("refuted_claims", []),
                "uncertain_claims": final_judgment.get("uncertain_claims", []),
                "nuanced_claims": final_judgment.get("nuanced_claims", [])  # æ–°å¢ï¼š nuanced ç»“è®º
            },
            
            # é‡è¦ä¿¡æºå¼•ç”¨
            "key_sources_cited": [
                {
                    "evidence_id": s.get("evidence_id"),
                    "title": s.get("title"),
                    "domain": s.get("source_domain"),
                    "credibility": s.get("source_credibility"),
                    "key_insight": s.get("key_insight", "")[:100],
                    "why_important": s.get("importance_note", "")
                }
                for s in key_sources[:5]
            ],
            
            # å¯è¿½æº¯æ—¥å¿—
            "traceability_log": {
                "agent_version": "2.0",
                "processing_steps": [
                    "multi_dimensional_decomposition",
                    "evidence_comprehensive_evaluation",
                    "conflict_resolution",
                    "multi_angle_synthesis",
                    "conclusion_generation"
                ],
                "decision_points": final_judgment.get("decision_points", []),
                "confidence_breakdown": final_judgment.get("confidence_breakdown", {})
            },
            
            "generated_at": str(uuid.uuid1()),
            "processing_time_ms": 3500
        }

    async def verdict_stream(self, search_result: Dict[str, Any], original_content: str) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼é‰´å®šï¼Œå®æ—¶è¿”å›å¤šç»´åº¦æ¨ç†è¿‡ç¨‹
        """
        verdict_id = str(uuid.uuid4())

        key_sources = search_result.get("key_sources", [])
        regular_sources = search_result.get("regular_sources", [])
        all_sources = search_result.get("all_sources", [])
        search_analysis = search_result.get("analysis", {})
        query_analysis = search_result.get("query_analysis", {})

        # å¼€å§‹é‰´å®š
        yield {
            "type": "reasoning",
            "agent": "verdict",
            "step": "é‰´å®šå¯åŠ¨",
            "content": f"ğŸ¯ å¯åŠ¨å¤šç»´åº¦æ·±åº¦é‰´å®š\n"
                       f"   ğŸ“Œ å…³é”®ä¿¡æº: {len(key_sources)} ä¸ª\n"
                       f"   ğŸ“„ æ™®é€šä¿¡æº: {len(regular_sources)} ä¸ª\n"
                       f"   ğŸ” Search Agent å·²è¯†åˆ« {len(search_analysis.get('key_findings', []))} ä¸ªæ ¸å¿ƒå‘ç°"
        }

        if not all_sources:
            yield {
                "type": "reasoning",
                "agent": "verdict",
                "step": "è¯æ®æ£€æŸ¥",
                "content": "âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³ä¿¡æºï¼Œæ— æ³•å®Œæˆé‰´å®š"
            }
            result = self._create_unverifiable_result(verdict_id, search_result.get("search_id"))
            yield {"type": "result", "agent": "verdict", "data": result}
            return

        # é˜¶æ®µ1: å¤šç»´åº¦é—®é¢˜åˆ†è§£
        yield {
            "type": "reasoning",
            "agent": "verdict",
            "step": "å¤šç»´åº¦åˆ†æ",
            "content": "ğŸ”¬ ä»å¤šç»´åº¦å®¡è§†é—®é¢˜...\n"
                       "   - äº‹å®ç»´åº¦ï¼šæ ¸å¿ƒä¸»å¼ æ˜¯å¦å±å®\n"
                       "   - èƒŒæ™¯ç»´åº¦ï¼šäº‹ä»¶çš„å‰å› åæœ\n"
                       "   - åŠ¨æœºç»´åº¦ï¼šä¿¡æ¯ä¼ æ’­çš„å¯èƒ½åŠ¨æœº\n"
                       "   - å½±å“ç»´åº¦ï¼šè¯¥ä¿¡æ¯çš„æ½œåœ¨å½±å“"
        }

        dimensions = await self._analyze_dimensions(
            original_content, query_analysis, search_analysis
        )

        for dim_name, dim_data in dimensions.items():
            yield {
                "type": "reasoning",
                "agent": "verdict",
                "step": f"{dim_name}ç»´åº¦",
                "content": f"ğŸ“Š {dim_name}ç»´åº¦åˆ†æ:\n{dim_data.get('analysis', '')}"
            }

        # é˜¶æ®µ2: è¯æ®è¯„ä¼°
        yield {
            "type": "reasoning",
            "agent": "verdict",
            "step": "è¯æ®è¯„ä¼°",
            "content": f"ğŸ§© æ·±åº¦è¯„ä¼°è¯æ®...\n"
                       f"   - è¯„ä¼° {len(key_sources)} ä¸ªå…³é”®ä¿¡æº\n"
                       f"   - è¯„ä¼° {len(regular_sources)} ä¸ªæ™®é€šä¿¡æº\n"
                       f"   - åˆ†æ Search Agent è¯†åˆ«çš„ {len(search_analysis.get('conflict_points', []))} ä¸ªå†²çªç‚¹"
        }

        evidence_evaluation = await self._evaluate_evidence_comprehensive(
            key_sources, regular_sources, search_analysis, original_content
        )

        yield {
            "type": "reasoning",
            "agent": "verdict",
            "step": "è¯æ®æƒé‡",
            "content": "âš–ï¸ è¯æ®æƒé‡åˆ†æ:\n" +
                       "\n".join([f"   â€¢ {e}" for e in evidence_evaluation.get("weight_analysis", [])])
        }

        # é˜¶æ®µ3: å¤šè§’åº¦ç»¼åˆ
        yield {
            "type": "reasoning",
            "agent": "verdict",
            "step": "ç»¼åˆåˆ¤æ–­",
            "content": "ğŸ­ ä»å¤šè§’åº¦ç»¼åˆåˆ¤æ–­...\n"
                       "   - å­—é¢æ„æ€ vs æ·±å±‚å«ä¹‰\n"
                       "   - ç›´æ¥è¯æ® vs é—´æ¥è¯æ®\n"
                       "   - çŸ­æœŸå½±å“ vs é•¿æœŸå½±å“\n"
                       "   - è¡¨é¢ç°è±¡ vs æœ¬è´¨é—®é¢˜"
        }

        final_judgment = await self._synthesize_judgment(
            original_content, dimensions, evidence_evaluation, search_analysis
        )

        # è¾“å‡ºå¤šè§’åº¦æ¨ç† - è¯¦ç»†åˆ†æ
        multi_angle = final_judgment.get("multi_angle_reasoning", {})

        yield {
            "type": "reasoning",
            "agent": "verdict",
            "step": "å¤šè§’åº¦åˆ†æ",
            "content": "ğŸ­ ä»å¤šè§’åº¦å®¡è§†é—®é¢˜...\n\n"
                       "   åˆ†æç»´åº¦:\n"
                       "   â€¢ å­—é¢æ„æ€ vs æ·±å±‚å«ä¹‰\n"
                       "   â€¢ ç›´æ¥è¯æ® vs é—´æ¥è¯æ®\n"
                       "   â€¢ çŸ­æœŸå½±å“ vs é•¿æœŸå½±å“\n"
                       "   â€¢ è¡¨é¢ç°è±¡ vs æœ¬è´¨é—®é¢˜"
        }

        angle_names = {
            "literal_meaning": "å­—é¢æ„æ€",
            "deep_implication": "æ·±å±‚å«ä¹‰",
            "direct_evidence": "ç›´æ¥è¯æ®",
            "indirect_evidence": "é—´æ¥è¯æ®",
            "short_term": "çŸ­æœŸå½±å“",
            "long_term": "é•¿æœŸå½±å“"
        }

        for angle, reasoning in multi_angle.items():
            angle_name = angle_names.get(angle, angle)
            yield {
                "type": "reasoning",
                "agent": "verdict",
                "step": f"è§’åº¦-{angle_name}",
                "content": f"ğŸ“ {angle_name}:\n   {reasoning}"
            }

        # è¾“å‡ºè¯¦ç»†æ¨ç†é“¾
        reasoning_chain = final_judgment.get("reasoning_chain", [])
        if reasoning_chain:
            yield {
                "type": "reasoning",
                "agent": "verdict",
                "step": "æ¨ç†é“¾æ¡",
                "content": "ğŸ§  è¯¦ç»†æ¨ç†è¿‡ç¨‹:\n\n" +
                           "\n".join([f"   æ­¥éª¤ {i+1}:\n      {r}\n" for i, r in enumerate(reasoning_chain)])
            }

        # å‘ç°åˆ†ç±»
        findings = {
            "verified_claims": final_judgment.get("verified_claims", []),
            "refuted_claims": final_judgment.get("refuted_claims", []),
            "uncertain_claims": final_judgment.get("uncertain_claims", []),
            "nuanced_claims": final_judgment.get("nuanced_claims", [])
        }

        yield {
            "type": "reasoning",
            "agent": "verdict",
            "step": "å‘ç°åˆ†ç±»",
            "content": "ğŸ“Š å‘ç°åˆ†ç±»:\n" +
                       f"   âœ… å·²éªŒè¯: {len(findings['verified_claims'])} é¡¹\n" +
                       f"   âŒ å·²è¯ä¼ª: {len(findings['refuted_claims'])} é¡¹\n" +
                       f"   âš ï¸ éœ€ nuanced ç†è§£: {len(findings['nuanced_claims'])} é¡¹\n" +
                       f"   â“ ä¸ç¡®å®š: {len(findings['uncertain_claims'])} é¡¹"
        }

        # ç»“è®ºç”Ÿæˆ - è¯¦ç»†è¯´æ˜
        conclusion = final_judgment.get("conclusion", "uncertain")
        confidence = final_judgment.get("confidence_score", 0.5)
        summary = final_judgment.get("summary", "")

        conclusion_emoji = {
            "true": "âœ…",
            "false": "âŒ",
            "uncertain": "âš ï¸",
            "unverifiable": "â“",
            "partially_true": "ğŸŸ¨",
            "misleading": "ğŸŸ§"
        }.get(conclusion, "â“")

        conclusion_labels = {
            "true": "çœŸå®",
            "false": "è™šå‡",
            "uncertain": "å­˜ç–‘",
            "unverifiable": "æ— æ³•æ ¸å®",
            "partially_true": "éƒ¨åˆ†çœŸå®",
            "misleading": "è¯¯å¯¼æ€§"
        }

        yield {
            "type": "reasoning",
            "agent": "verdict",
            "step": "ç»“è®ºç”Ÿæˆ",
            "content": f"ğŸ¯ æœ€ç»ˆé‰´å®šç»“è®º\n\n"
                       f"   {conclusion_emoji} ç»“è®º: {conclusion_labels.get(conclusion, conclusion)}\n"
                       f"   ğŸ“Š ç½®ä¿¡åº¦: {confidence:.0%}\n"
                       f"   ğŸ“ ç»“è®ºæ‘˜è¦:\n      {summary}\n\n"
                       f"   ğŸ’¡ ç»“è®ºä¾æ®:\n"
                       f"      â€¢ åŸºäº {len(key_sources)} ä¸ªå…³é”®ä¿¡æº\n"
                       f"      â€¢ ç»¼åˆ {len(regular_sources)} ä¸ªæ™®é€šä¿¡æº\n"
                       f"      â€¢ å¤šç»´åº¦äº¤å‰éªŒè¯"
        }

        # æ„å»ºæœ€ç»ˆç»“æœ
        evidence_chain = self._build_comprehensive_evidence_chain(
            key_sources, regular_sources, final_judgment.get("supporting_sources", [])
        )

        final_result = {
            "verdict_id": verdict_id,
            "search_task_ref": search_result.get("search_id"),
            "conclusion": conclusion,
            "confidence_score": confidence,
            "conclusion_summary": summary,
            "dimensional_analysis": {
                "factual_dimension": dimensions.get("factual", {}),
                "contextual_dimension": dimensions.get("contextual", {}),
                "motivational_dimension": dimensions.get("motivational", {}),
                "impact_dimension": dimensions.get("impact", {})
            },
            "reasoning_chain": reasoning_chain,
            "multi_angle_reasoning": multi_angle,
            "evidence_evaluation": evidence_evaluation,
            "evidence_chain": evidence_chain,
            "findings": {
                "verified_claims": final_judgment.get("verified_claims", []),
                "refuted_claims": final_judgment.get("refuted_claims", []),
                "uncertain_claims": final_judgment.get("uncertain_claims", []),
                "nuanced_claims": final_judgment.get("nuanced_claims", [])
            },
            "key_sources_cited": [
                {
                    "evidence_id": s.get("evidence_id"),
                    "title": s.get("title"),
                    "domain": s.get("source_domain"),
                    "credibility": s.get("source_credibility"),
                    "key_insight": s.get("key_insight", "")[:100],
                    "why_important": s.get("importance_note", "")
                }
                for s in key_sources[:5]
            ],
            "traceability_log": {
                "agent_version": "2.0",
                "processing_steps": [
                    "multi_dimensional_decomposition",
                    "evidence_comprehensive_evaluation",
                    "conflict_resolution",
                    "multi_angle_synthesis",
                    "conclusion_generation"
                ],
                "decision_points": final_judgment.get("decision_points", []),
                "confidence_breakdown": final_judgment.get("confidence_breakdown", {})
            },
            "generated_at": str(uuid.uuid1()),
            "processing_time_ms": 3500
        }

        yield {
            "type": "result",
            "agent": "verdict",
            "data": final_result
        }

    async def _analyze_dimensions(self, original_content: str, query_analysis: Dict, search_analysis: Dict) -> Dict[str, Any]:
        """
        å¤šç»´åº¦é—®é¢˜åˆ†è§£åˆ†æ
        """
        key_findings = search_analysis.get("key_findings", [])
        perspectives = search_analysis.get("perspectives", {})

        prompt = f"""ä½ æ˜¯ä¸€ä½å¤šç»´åº¦åˆ†æä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å››ä¸ªç»´åº¦æ·±åº¦åˆ†æè¿™ä¸ªé—®é¢˜ï¼š

ã€å¾…åˆ†æå†…å®¹ã€‘
{original_content}

ã€æ ¸å¿ƒé—®é¢˜ã€‘
{query_analysis.get('core_question', '')}

ã€Search Agent çš„æ ¸å¿ƒå‘ç°ã€‘
{chr(10).join(['- ' + f for f in key_findings[:5]])}

ã€ä¸åŒç«‹åœºçš„è§‚ç‚¹ã€‘
æ”¯æŒæ–¹: {perspectives.get('supporting', 'æ— ')}
åå¯¹æ–¹: {perspectives.get('opposing', 'æ— ')}
ä¸­ç«‹æ–¹: {perspectives.get('neutral', 'æ— ')}

è¯·ä»ä»¥ä¸‹å››ä¸ªç»´åº¦è¿›è¡Œåˆ†æï¼Œè¿”å›JSONæ ¼å¼ï¼š
{{
    "factual": {{
        "analysis": "äº‹å®ç»´åº¦åˆ†æï¼šæ ¸å¿ƒä¸»å¼ çš„äº‹å®åŸºç¡€ã€å¯éªŒè¯æ€§ã€è¯æ®å¼ºåº¦",
        "key_points": ["å…³é”®ç‚¹1", "å…³é”®ç‚¹2"],
        "confidence": 0.85
    }},
    "contextual": {{
        "analysis": "èƒŒæ™¯ç»´åº¦åˆ†æï¼šäº‹ä»¶çš„å‰å› åæœã€å†å²èƒŒæ™¯ã€ç›¸å…³äº‹ä»¶",
        "key_points": ["èƒŒæ™¯ç‚¹1", "èƒŒæ™¯ç‚¹2"],
        "confidence": 0.75
    }},
    "motivational": {{
        "analysis": "åŠ¨æœºç»´åº¦åˆ†æï¼šä¿¡æ¯ä¼ æ’­çš„å¯èƒ½åŠ¨æœºã€åˆ©ç›Šç›¸å…³æ–¹ã€å™äº‹ç›®çš„",
        "key_points": ["åŠ¨æœºç‚¹1", "åŠ¨æœºç‚¹2"],
        "confidence": 0.60
    }},
    "impact": {{
        "analysis": "å½±å“ç»´åº¦åˆ†æï¼šè¯¥ä¿¡æ¯çš„æ½œåœ¨ç¤¾ä¼šå½±å“ã€æƒ…ç»ªå½±å“ã€è¡Œä¸ºå½±å“",
        "key_points": ["å½±å“ç‚¹1", "å½±å“ç‚¹2"],
        "confidence": 0.70
    }}
}}

è¦æ±‚ï¼š
1. æ¯ä¸ªç»´åº¦éƒ½è¦æœ‰æ·±å…¥çš„åˆ†æ
2. æŒ‡å‡ºè¯¥ç»´åº¦çš„å…³é”®è¯æ®æ”¯æ’‘
3. ç»™å‡ºè¯¥ç»´åº¦çš„ç½®ä¿¡åº¦è¯„ä¼°
4. æ€è€ƒç»´åº¦ä¹‹é—´çš„å…³è”"""

        try:
            result_text = await self._call_llm(prompt)
            return self._parse_llm_response(result_text)
        except Exception as e:
            print(f"[VerdictAgent] Dimension analysis error: {e}")
            return {
                "factual": {"analysis": "åˆ†æå¤±è´¥", "key_points": [], "confidence": 0.5},
                "contextual": {"analysis": "åˆ†æå¤±è´¥", "key_points": [], "confidence": 0.5},
                "motivational": {"analysis": "åˆ†æå¤±è´¥", "key_points": [], "confidence": 0.5},
                "impact": {"analysis": "åˆ†æå¤±è´¥", "key_points": [], "confidence": 0.5}
            }

    async def _evaluate_evidence_comprehensive(self, key_sources: List[Dict], regular_sources: List[Dict], 
                                                search_analysis: Dict, original_content: str) -> Dict[str, Any]:
        """
        ç»¼åˆè¯„ä¼°æ‰€æœ‰è¯æ®
        """
        conflict_points = search_analysis.get("conflict_points", [])
        evidence_gaps = search_analysis.get("evidence_gaps", [])

        # å‡†å¤‡ä¿¡æºæ‘˜è¦
        key_sources_summary = []
        for s in key_sources[:6]:
            key_sources_summary.append({
                "domain": s.get("source_domain", ""),
                "credibility": s.get("source_credibility", "medium"),
                "stance": s.get("source_stance", "neutral"),
                "insight": s.get("key_insight", "")[:120],
                "deep_analysis": s.get("deep_analysis", "")[:80]
            })

        prompt = f"""ä½ æ˜¯ä¸€ä½è¯æ®è¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹è¯æ®è¿›è¡Œç»¼åˆè¯„ä¼°ã€‚

ã€å¾…æ ¸å®å†…å®¹ã€‘
{original_content}

ã€å…³é”®ä¿¡æºã€‘
{json.dumps(key_sources_summary, ensure_ascii=False, indent=2)}

ã€Search Agent è¯†åˆ«çš„å†²çªç‚¹ã€‘
{chr(10).join(['- ' + c for c in conflict_points[:4]])}

ã€è¯æ®ç¼ºå£ã€‘
{chr(10).join(['- ' + g for g in evidence_gaps[:3]])}

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "key_sources_assessment": [
        {{
            "domain": "ä¿¡æºåŸŸå",
            "assessment": "å¯¹è¯¥ä¿¡æºçš„è¯„ä¼°",
            "weight": 0.9,
            "reliability_concerns": "å¯é æ€§æ‹…å¿§ï¼ˆå¦‚æœ‰ï¼‰"
        }}
    ],
    "conflict_resolution": "å¦‚ä½•å¤„ç†ä¿¡æºä¹‹é—´çš„å†²çª",
    "weight_analysis": [
        "è¯æ®æƒé‡åˆ†æç‚¹1",
        "è¯æ®æƒé‡åˆ†æç‚¹2"
    ],
    "evidence_strength": 0.75,
    "coverage_assessment": "è¯æ®è¦†ç›–åº¦è¯„ä¼°",
    "overall_quality": "æ•´ä½“è¯æ®è´¨é‡è¯„ä»·"
}}

è¦æ±‚ï¼š
1. è¯¦ç»†è¯„ä¼°æ¯ä¸ªå…³é”®ä¿¡æº
2. è¯´æ˜å¦‚ä½•å¤„ç†å†²çªä¿¡æ¯
3. è¯„ä¼°è¯æ®çš„æ•´ä½“å¼ºåº¦å’Œè¦†ç›–åº¦
4. æŒ‡å‡ºä»»ä½•å¯é æ€§æ‹…å¿§"""

        try:
            result_text = await self._call_llm(prompt)
            return self._parse_llm_response(result_text)
        except Exception as e:
            print(f"[VerdictAgent] Evidence evaluation error: {e}")
            return {
                "key_sources_assessment": [],
                "conflict_resolution": "è¯„ä¼°å¤±è´¥",
                "weight_analysis": [],
                "evidence_strength": 0.5,
                "coverage_assessment": "è¯„ä¼°å¤±è´¥",
                "overall_quality": "è¯„ä¼°å¤±è´¥"
            }

    async def _synthesize_judgment(self, original_content: str, dimensions: Dict, 
                                   evidence_evaluation: Dict, search_analysis: Dict) -> Dict[str, Any]:
        """
        ç»¼åˆå¤šè§’åº¦åˆ¤æ–­ï¼Œç”Ÿæˆæœ€ç»ˆç»“è®º
        """
        key_findings = search_analysis.get("key_findings", [])
        analysis_reasoning = search_analysis.get("analysis_reasoning", "")
        perspectives = search_analysis.get("perspectives", {})

        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº‹å®æ ¸æŸ¥ä¸“å®¶ã€‚è¯·åŸºäºå¤šç»´åº¦åˆ†æå’Œè¯æ®è¯„ä¼°ï¼Œç”Ÿæˆæœ€ç»ˆçš„ç»¼åˆåˆ¤æ–­ã€‚

ã€å¾…æ ¸å®å†…å®¹ã€‘
{original_content}

ã€å¤šç»´åº¦åˆ†æç»“æœã€‘
äº‹å®ç»´åº¦ç½®ä¿¡åº¦: {dimensions.get('factual', {}).get('confidence', 0.5)}
èƒŒæ™¯ç»´åº¦ç½®ä¿¡åº¦: {dimensions.get('contextual', {}).get('confidence', 0.5)}
åŠ¨æœºç»´åº¦ç½®ä¿¡åº¦: {dimensions.get('motivational', {}).get('confidence', 0.5)}
å½±å“ç»´åº¦ç½®ä¿¡åº¦: {dimensions.get('impact', {}).get('confidence', 0.5)}

ã€è¯æ®è¯„ä¼°ã€‘
æ•´ä½“è¯æ®å¼ºåº¦: {evidence_evaluation.get('evidence_strength', 0.5)}
è¯æ®è´¨é‡: {evidence_evaluation.get('overall_quality', 'æœªçŸ¥')}

ã€Search Agent çš„åˆ†ææ¨ç†ã€‘
{analysis_reasoning[:500]}

ã€æ ¸å¿ƒå‘ç°ã€‘
{chr(10).join(['- ' + f for f in key_findings[:5]])}

ã€ä¸åŒç«‹åœºè§‚ç‚¹ã€‘
æ”¯æŒæ–¹: {perspectives.get('supporting', 'æ— ')[:200]}
åå¯¹æ–¹: {perspectives.get('opposing', 'æ— ')[:200]}

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "conclusion": "true|false|uncertain|unverifiable|partially_true|misleading",
    "confidence_score": 0.85,
    "summary": "é‰´å®šç»“è®ºæ‘˜è¦ï¼ˆ150å­—ä»¥å†…ï¼Œè¦æœ‰è¯´æœåŠ›ï¼‰",
    "reasoning_chain": [
        "æ¨ç†æ­¥éª¤1ï¼šä»äº‹å®ç»´åº¦çš„åˆ†æ",
        "æ¨ç†æ­¥éª¤2ï¼šå¦‚ä½•å¤„ç†è¯æ®å†²çª",
        "æ¨ç†æ­¥éª¤3ï¼šå¤šè§’åº¦ç»¼åˆåˆ¤æ–­",
        "æ¨ç†æ­¥éª¤4ï¼šæœ€ç»ˆç»“è®ºçš„å½¢æˆ"
    ],
    "multi_angle_reasoning": {{
        "literal_meaning": "å­—é¢æ„æ€çš„åˆ†æ",
        "deep_implication": "æ·±å±‚å«ä¹‰çš„æŒ–æ˜",
        "direct_evidence": "ç›´æ¥è¯æ®çš„è¯„ä¼°",
        "indirect_evidence": "é—´æ¥è¯æ®çš„è¯„ä¼°",
        "short_term": "çŸ­æœŸå½±å“åˆ†æ",
        "long_term": "é•¿æœŸå½±å“åˆ†æ"
    }},
    "verified_claims": ["å·²éªŒè¯çš„å…·ä½“ä¸»å¼ "],
    "refuted_claims": ["å·²è¯ä¼ªçš„å…·ä½“ä¸»å¼ "],
    "uncertain_claims": ["ä¸ç¡®å®šçš„ä¸»å¼ "],
    "nuanced_claims": ["éœ€è¦ nuanced ç†è§£çš„ä¸»å¼ ï¼Œå¦‚'éƒ¨åˆ†çœŸå®ä½†æœ‰è¯¯å¯¼æ€§'"],
    "supporting_sources": ["evidence_id1", "evidence_id2"],
    "decision_points": [
        {{"step": "è¯æ®è¯„ä¼°", "decision": "é«˜å¯ä¿¡åº¦ä¿¡æºå ä¸»å¯¼"}},
        {{"step": "å†²çªå¤„ç†", "decision": "é‡‡ç”¨å¤šæ•°é«˜å¯ä¿¡åº¦ä¿¡æºçš„è§‚ç‚¹"}}
    ],
    "confidence_breakdown": {{
        "factual_basis": 0.9,
        "evidence_quality": 0.85,
        "source_credibility": 0.88,
        "consistency": 0.82
    }}
}}

ç»“è®ºç±»å‹è¯´æ˜ï¼š
- true: å†…å®¹å±å®ï¼Œæœ‰å¤šä¸ªé«˜å¯ä¿¡åº¦ä¿¡æºè¯å®
- false: å†…å®¹è™šå‡ï¼Œæœ‰æ˜ç¡®è¯æ®è¯ä¼ª
- uncertain: å­˜ç–‘ï¼Œè¯æ®ä¸è¶³æˆ–ç›¸äº’çŸ›ç›¾
- unverifiable: æ— æ³•æ ¸å®ï¼Œç¼ºä¹å¯éªŒè¯çš„å®¢è§‚ä¾æ®
- partially_true: éƒ¨åˆ†çœŸå®ï¼Œä½†å­˜åœ¨å¤¸å¤§æˆ–é—æ¼
- misleading: å…·æœ‰è¯¯å¯¼æ€§ï¼Œè™½ç„¶å­—é¢å¯èƒ½æ­£ç¡®ä½†å¼•å¯¼é”™è¯¯ç»“è®º

è¦æ±‚ï¼š
1. ç»“è®ºè¦æœ‰è¯´æœåŠ›ï¼ŒåŸºäºå……åˆ†çš„è¯æ®
2. ä¸è¦å±€é™äºå­—é¢æ„æ€ï¼Œè¦æŒ–æ˜æ·±å±‚å«ä¹‰
3. æ‰¿è®¤ä¸ç¡®å®šæ€§ï¼Œä¸è¦è¿‡åº¦è‡ªä¿¡
4. ç»™å‡ºè¯¦ç»†çš„æ¨ç†è¿‡ç¨‹
5. è€ƒè™‘ä¸åŒè§’åº¦çš„è§‚ç‚¹"""

        try:
            result_text = await self._call_llm(prompt)
            return self._parse_llm_response(result_text)
        except Exception as e:
            print(f"[VerdictAgent] Judgment synthesis error: {e}")
            return {
                "conclusion": "uncertain",
                "confidence_score": 0.5,
                "summary": "ç»¼åˆåˆ¤æ–­è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯",
                "reasoning_chain": ["é”™è¯¯: " + str(e)],
                "multi_angle_reasoning": {},
                "verified_claims": [],
                "refuted_claims": [],
                "uncertain_claims": [original_content],
                "nuanced_claims": [],
                "supporting_sources": [],
                "decision_points": [],
                "confidence_breakdown": {}
            }

    async def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨ LLM"""
        try:
            if self.llm_provider == "openai" and self.openai_client:
                print(f"[VerdictAgent] Calling OpenAI API with model: {self.model}")
                response = await self.openai_client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº‹å®æ ¸æŸ¥ä¸“å®¶å’Œæ‰¹åˆ¤æ€§æ€ç»´å¯¼å¸ˆã€‚ä½ æ“…é•¿å¤šç»´åº¦åˆ†æã€å¤šè§’åº¦æ€è€ƒï¼Œä¸å±€é™äºè¡¨é¢ç°è±¡ã€‚ä½ æ€»æ˜¯åŸºäºè¯æ®è¯´è¯ï¼Œå–„äºå‘ç°é—®é¢˜çš„å¤æ‚æ€§ï¼Œç»™å‡º nuanced çš„ç»“è®ºã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=3000
                )
                content = response.choices[0].message.content
                print(f"[VerdictAgent] LLM Response received: {content[:200]}...")
                return content
            elif self.llm_provider == "claude" and self.anthropic_client:
                print(f"[VerdictAgent] Calling Claude API")
                response = self.anthropic_client.messages.create(
                    model=settings.ANTHROPIC_MODEL,
                    max_tokens=3500,
                    temperature=self.temperature,
                    messages=[{"role": "user", "content": prompt}]
                )
                content = response.content[0].text
                print(f"[VerdictAgent] LLM Response received: {content[:200]}...")
                return content
            else:
                print(f"[VerdictAgent] Warning: No LLM client available")
                return self._create_fallback_response()
        except asyncio.TimeoutError:
            print(f"[VerdictAgent] LLM Timeout Error")
            return self._create_fallback_response()
        except Exception as e:
            print(f"[VerdictAgent] LLM Error: {str(e)}")
            return self._create_fallback_response()

    def _parse_llm_response(self, result_text: str) -> Dict[str, Any]:
        """è§£æ LLM å“åº”"""
        if not result_text or not result_text.strip():
            return {}

        text = result_text.strip()

        if text.startswith("```json"):
            text = text[7:]
        elif text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]

        text = text.strip()

        try:
            result = json.loads(text)
            return result
        except json.JSONDecodeError as e:
            print(f"[VerdictAgent] JSON Parse Error: {str(e)}")
            # å°è¯•æå– JSON
            try:
                start = text.find("{")
                end = text.rfind("}") + 1
                if start >= 0 and end > start:
                    return json.loads(text[start:end])
            except:
                pass
            return {}

    def _create_fallback_response(self) -> str:
        """åˆ›å»ºé™çº§å“åº”"""
        return json.dumps({
            "conclusion": "uncertain",
            "confidence_score": 0.5,
            "summary": "ç”±äºæŠ€æœ¯åŸå› ï¼Œæ— æ³•å®Œæˆé‰´å®šã€‚",
            "reasoning_chain": ["ç³»ç»Ÿæš‚æ—¶æ— æ³•è®¿é—®é‰´å®šæœåŠ¡"],
            "multi_angle_reasoning": {},
            "verified_claims": [],
            "refuted_claims": [],
            "uncertain_claims": ["å¾…é‰´å®šå†…å®¹"],
            "nuanced_claims": [],
            "supporting_sources": [],
            "decision_points": [],
            "confidence_breakdown": {}
        })

    def _build_comprehensive_evidence_chain(self, key_sources: List[Dict], regular_sources: List[Dict], 
                                            supporting_ids: List[str]) -> List[Dict[str, Any]]:
        """æ„å»ºç»¼åˆè¯æ®é“¾"""
        evidence_chain = []
        all_sources = key_sources + regular_sources

        for source in all_sources:
            evidence_id = source.get("evidence_id")
            supports = evidence_id in supporting_ids if supporting_ids else True

            evidence_chain.append({
                "evidence_id": evidence_id,
                "source_ref": source.get("source_url"),
                "source_domain": source.get("source_domain"),
                "source_credibility": source.get("source_credibility"),
                "is_key_source": source.get("is_key_source", False),
                "claim_ref": "c1",
                "supports": supports,
                "weight": self._calculate_weight(source),
                "reason": source.get("key_insight", "")[:100] if supports else source.get("deep_analysis", "")[:100]
            })

        return evidence_chain

    def _calculate_weight(self, source: Dict) -> float:
        """è®¡ç®—è¯æ®æƒé‡"""
        weight = 0.5

        # å¯ä¿¡åº¦æƒé‡
        credibility_scores = {"high": 0.9, "medium": 0.6, "low": 0.3}
        weight *= credibility_scores.get(source.get("source_credibility", "medium"), 0.5)

        # ç›¸å…³åº¦æƒé‡
        weight *= source.get("relevance_score", 0.8)

        # å…³é”®ä¿¡æºåŠ æˆ
        if source.get("is_key_source"):
            weight *= 1.2

        return min(1.0, weight)

    def _create_unverifiable_result(self, verdict_id: str, search_id: str) -> Dict[str, Any]:
        """åˆ›å»ºæ— æ³•æ ¸å®çš„ç»“æœ"""
        return {
            "verdict_id": verdict_id,
            "search_task_ref": search_id,
            "conclusion": "unverifiable",
            "confidence_score": 0.0,
            "conclusion_summary": "æœªæ‰¾åˆ°ç›¸å…³è¯æ®ï¼Œæ— æ³•æ ¸å®è¯¥å†…å®¹çš„çœŸå®æ€§ã€‚",
            "dimensional_analysis": {},
            "reasoning_chain": ["æœªæ£€ç´¢åˆ°ç›¸å…³ä¿¡æº", "ç¼ºä¹å¯éªŒè¯çš„å®¢è§‚ä¾æ®"],
            "multi_angle_reasoning": {},
            "evidence_evaluation": {},
            "evidence_chain": [],
            "findings": {
                "verified_claims": [],
                "refuted_claims": [],
                "uncertain_claims": ["å¾…é‰´å®šå†…å®¹"],
                "nuanced_claims": []
            },
            "key_sources_cited": [],
            "traceability_log": {
                "agent_version": "2.0",
                "processing_steps": ["search"],
                "decision_points": [{"step": "search", "decision": "no_evidence_found"}],
                "confidence_breakdown": {}
            },
            "generated_at": str(uuid.uuid1()),
            "processing_time_ms": 0
        }
