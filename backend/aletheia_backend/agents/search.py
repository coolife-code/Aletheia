import json
import uuid
from typing import List, Dict, Any, AsyncGenerator
import openai
from anthropic import Anthropic
import asyncio

from aletheia_backend.core.config import settings


class SearchAgent:
    """
    æœç´¢åˆ†æ Agent - ä¸“ä¸šçš„ä¿¡æ¯åˆ†æå¸ˆå’Œæ‰¾èŒ¬ä¸“å®¶
    
    èŒè´£ï¼š
    1. æ·±åº¦ç†è§£é—®é¢˜çš„æ ¸å¿ƒçŸ›ç›¾å’Œå…³é”®ç‚¹
    2. æ‰§è¡Œç²¾å‡†æœç´¢ï¼Œæ”¶é›†å¤šæ–¹è¯æ®
    3. åˆ†æä¿¡æºçš„å¯ä¿¡åº¦ã€ç«‹åœºå’Œæ½œåœ¨åè§
    4. è¯†åˆ«ä¿¡æ¯å†²çªç‚¹å’Œçªç ´å£
    5. ç­›é€‰æœ€å…³é”®çš„ä¿¡æºï¼Œæ ‡è®°é‡è¦è¯æ®
    """

    def __init__(self):
        self.llm_provider = settings.LLM_PROVIDER
        self.openai_client = openai.AsyncOpenAI(
            api_key=settings.OPENAI_API_KEY,
            base_url=settings.OPENAI_BASE_URL,
            timeout=120.0
        ) if settings.OPENAI_API_KEY else None
        self.anthropic_client = Anthropic(
            api_key=settings.ANTHROPIC_API_KEY
        ) if settings.ANTHROPIC_API_KEY else None
        print(f"[SearchAgent] Initialized with LLM provider: {self.llm_provider}")

    async def search(self, parser_result: Dict[str, Any], original_content: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ·±åº¦æœç´¢å’Œåˆ†æ

        Args:
            parser_result: Parser Agent çš„è¾“å‡ºï¼ŒåŒ…å«æœç´¢æŸ¥è¯¢åˆ—è¡¨å’Œåˆ†æ
            original_content: ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢å†…å®¹

        Returns:
            åŒ…å«æ·±åº¦åˆ†æçš„ä¿¡æºæ•°æ®é›†
        """
        search_id = str(uuid.uuid4())
        search_queries = parser_result.get("search_queries", [])
        query_analysis = parser_result.get("analysis", {})

        print(f"[SearchAgent] Starting deep analysis with {len(search_queries)} queries")

        # é˜¶æ®µ1: æ‰§è¡Œå¤šæ¬¡æœç´¢æ”¶é›†è¯æ®
        all_sources = []
        query_reasoning = []

        for i, query in enumerate(search_queries[:4]):
            print(f"[SearchAgent] Query {i+1}/{len(search_queries)}: {query}")

            result = await self._execute_web_search(query, original_content, query_analysis)
            sources = result.get("sources", [])
            reasoning = result.get("search_reasoning", "")

            all_sources.extend(sources)
            if reasoning:
                query_reasoning.append({
                    "query": query,
                    "reasoning": reasoning
                })

            print(f"[SearchAgent] Query {i+1} returned {len(sources)} sources")

        # é˜¶æ®µ2: æ·±åº¦åˆ†æä¿¡æº
        print(f"[SearchAgent] Starting source analysis...")
        analyzed_sources = await self._analyze_sources_deep(
            all_sources, original_content, query_analysis
        )

        # é˜¶æ®µ3: è¯†åˆ«å…³é”®è¯æ®å’Œçªç ´å£
        print(f"[SearchAgent] Identifying key evidence...")
        key_findings = await self._identify_key_findings(
            analyzed_sources, original_content, query_analysis
        )

        # é˜¶æ®µ4: æ•´ç†è¾“å‡º
        unique_sources = self._deduplicate_sources(analyzed_sources)
        ranked_sources = self._rank_sources_by_importance(unique_sources, key_findings)

        # åˆ†ç¦»å…³é”®ä¿¡æºå’Œæ™®é€šä¿¡æº
        key_sources = [s for s in ranked_sources if s.get("is_key_source", False)][:8]
        regular_sources = [s for s in ranked_sources if not s.get("is_key_source", False)][:12]

        print(f"[SearchAgent] Analysis complete: {len(key_sources)} key sources, {len(regular_sources)} regular sources")

        return {
            "search_id": search_id,
            "parser_task_ref": parser_result.get("task_id"),
            "original_query": original_content,
            "query_analysis": query_analysis,
            
            # å…³é”®ä¿¡æºï¼ˆæœ€é‡è¦çš„çªç ´å£ï¼‰
            "key_sources": key_sources,
            
            # æ™®é€šä¿¡æº
            "regular_sources": regular_sources,
            
            # æ‰€æœ‰ä¿¡æºï¼ˆåˆå¹¶ï¼‰
            "all_sources": ranked_sources[:20],
            
            # æ·±åº¦åˆ†æç»“æœ
            "analysis": {
                # æœç´¢è¿‡ç¨‹æ¨ç†
                "search_reasoning_chain": query_reasoning,
                
                # æ ¸å¿ƒå‘ç°
                "key_findings": key_findings.get("findings", []),
                
                # ä¿¡æ¯å†²çªç‚¹
                "conflict_points": key_findings.get("conflict_points", []),
                
                # è¯æ®ç¼ºå£
                "evidence_gaps": key_findings.get("evidence_gaps", []),
                
                # åˆ†ææ¨ç†è¿‡ç¨‹
                "analysis_reasoning": key_findings.get("analysis_reasoning", ""),
                
                # å¤šè§’åº¦è§‚ç‚¹æ±‡æ€»
                "perspectives": key_findings.get("perspectives", {})
            },
            
            # å…ƒæ•°æ®
            "search_metadata": {
                "total_queries": len(search_queries),
                "executed_queries": min(len(search_queries), 4),
                "sources_found": len(all_sources),
                "sources_after_dedup": len(unique_sources),
                "key_sources_count": len(key_sources),
                "coverage_score": min(0.95, 0.5 + len(unique_sources) * 0.03),
                "analysis_depth": "deep",
                "search_duration_ms": 8000
            }
        }

    async def search_stream(self, parser_result: Dict[str, Any], original_content: str) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼æœç´¢åˆ†æï¼Œå®æ—¶è¿”å›æ¨ç†è¿‡ç¨‹
        """
        search_id = str(uuid.uuid4())
        search_queries = parser_result.get("search_queries", [])
        query_analysis = parser_result.get("analysis", {})

        # å¼€å§‹åˆ†æ - è¯¦ç»†æ¨ç†è¿‡ç¨‹
        core_entities = query_analysis.get('core_entities', [])
        core_question = query_analysis.get('core_question', '')
        info_types = query_analysis.get('info_types', [])

        yield {
            "type": "reasoning",
            "agent": "search",
            "step": "é—®é¢˜ç†è§£",
            "content": f"ğŸ§  Search Agent å¼€å§‹æ·±åº¦åˆ†æ...\n\n"
                       f"ğŸ“Œ æ ¸å¿ƒå®ä½“è¯†åˆ«:\n" +
                       chr(10).join([f"   â€¢ {e}" for e in core_entities]) +
                       f"\n\nğŸ¯ æ ¸å¿ƒé—®é¢˜:\n   {core_question}\n\n"
                       f"ğŸ“‹ ä¿¡æ¯ç±»å‹: {', '.join(info_types)}\n\n"
                       f"ğŸ’­ åˆ†ææ€è·¯:\n"
                       f"   1. ç†è§£é—®é¢˜çš„æ ¸å¿ƒçŸ›ç›¾å’Œå…³é”®ç‚¹\n"
                       f"   2. è®¾è®¡ç²¾å‡†æœç´¢ç­–ç•¥ï¼Œè¦†ç›–å¤šè§’åº¦\n"
                       f"   3. è¯„ä¼°æ¯ä¸ªä¿¡æºçš„å¯ä¿¡åº¦å’Œç«‹åœº\n"
                       f"   4. è¯†åˆ«ä¿¡æ¯å†²çªå’Œçªç ´å£"
        }

        all_sources = []
        query_reasoning = []

        # æ‰§è¡Œå¤šæ¬¡æœç´¢
        for i, query in enumerate(search_queries[:4]):
            yield {
                "type": "reasoning",
                "agent": "search",
                "step": f"æœç´¢{i+1}",
                "content": f"ğŸ” æ‰§è¡Œç¬¬ {i+1} è½®æœç´¢...\n\n"
                           f"ğŸ“¡ æœç´¢æŸ¥è¯¢:\n   {query}\n\n"
                           f"ğŸ’¡ æœç´¢ç­–ç•¥:\n"
                           f"   â€¢ ä½¿ç”¨è”ç½‘æœç´¢è·å–å®æ—¶ä¿¡æ¯\n"
                           f"   â€¢ ç­›é€‰é«˜å¯ä¿¡åº¦ä¿¡æº\n"
                           f"   â€¢ è®°å½•æœç´¢æ€è·¯å’Œå…³é”®å‘ç°"
            }

            result = await self._execute_web_search(query, original_content, query_analysis)
            sources = result.get("sources", [])
            reasoning = result.get("search_reasoning", "")

            all_sources.extend(sources)
            if reasoning:
                query_reasoning.append({
                    "query": query,
                    "reasoning": reasoning
                })

            yield {
                "type": "reasoning",
                "agent": "search",
                "step": f"æœç´¢{i+1}ç»“æœ",
                "content": f"âœ“ ç¬¬ {i+1} è½®æœç´¢å®Œæˆ\n"
                           f"   ğŸ“Š æ‰¾åˆ° {len(sources)} ä¸ªä¿¡æº\n"
                           f"   ğŸ’­ æœç´¢æ€è·¯: {reasoning}"
            }

            # æ˜¾ç¤ºæ¯ä¸ªä¿¡æºçš„è¯¦ç»†åˆ†æ
            for j, source in enumerate(sources, 1):
                credibility = source.get("source_credibility", "medium")
                credibility_emoji = "ğŸŸ¢" if credibility == "high" else "ğŸŸ¡" if credibility == "medium" else "ğŸ”´"
                credibility_text = "é«˜" if credibility == "high" else "ä¸­" if credibility == "medium" else "ä½"
                domain = source.get('source_domain', 'æœªçŸ¥')
                title = source.get('title', '')
                insight = source.get('key_insight', '')

                yield {
                    "type": "reasoning",
                    "agent": "search",
                    "step": f"ä¿¡æº{i+1}-{j}",
                    "content": f"   {credibility_emoji} ä¿¡æº {j}: [{credibility_text}å¯ä¿¡åº¦]\n"
                               f"      æ¥æº: {domain}\n"
                               f"      æ ‡é¢˜: {title}\n"
                               f"      å…³é”®ä¿¡æ¯: {insight}"
                }

            await asyncio.sleep(0.1)

        # æ·±åº¦åˆ†æé˜¶æ®µ - åªåˆ†ææœ€é‡è¦çš„4-5ç¯‡
        yield {
            "type": "reasoning",
            "agent": "search",
            "step": "ç²¾é€‰ç²¾è¯»",
            "content": f"ğŸ¯ ä» {len(all_sources)} ä¸ªä¿¡æºä¸­ç²¾é€‰æœ€é‡è¦çš„è¿›è¡Œç²¾è¯»...\n"
                       f"   - èšç„¦æ ¸å¿ƒé—®é¢˜ï¼š{query_analysis.get('core_question', '')[:50]}...\n"
                       f"   - åªç²¾è¯»4-5ç¯‡æœ€å…³é”®çš„ä¿¡æº\n"
                       f"   - é¿å…è¿‡åº¦åˆ†æï¼ŒæœåŠ¡äºæ ¸å¿ƒç›®æ ‡"
        }

        analyzed_sources = await self._analyze_sources_deep(
            all_sources, original_content, query_analysis
        )

        deep_analyzed_count = sum(1 for s in analyzed_sources if s.get('is_deep_analyzed'))
        yield {
            "type": "reasoning",
            "agent": "search",
            "step": "ç²¾è¯»å®Œæˆ",
            "content": f"âœ… ç²¾é€‰ç²¾è¯»å®Œæˆ:\n"
                       f"   - ç²¾è¯»ä¿¡æº: {deep_analyzed_count} ç¯‡\n"
                       f"   - å…¶ä»–ä¿¡æº: {len(analyzed_sources) - deep_analyzed_count} ç¯‡ï¼ˆåŸºç¡€ä¿¡æ¯ï¼‰\n"
                       f"   - èšç„¦æ ¸å¿ƒé—®é¢˜ï¼Œé¿å…å‘æ•£"
        }

        # è¯†åˆ«å…³é”®å‘ç°
        key_findings = await self._identify_key_findings(
            analyzed_sources, original_content, query_analysis
        )

        yield {
            "type": "reasoning",
            "agent": "search",
            "step": "å…³é”®å‘ç°",
            "content": f"ğŸ¯ å…³é”®å‘ç°:\n" +
                       "\n".join([f"   {i+1}. {f}" for i, f in enumerate(key_findings.get("findings", []))])
        }

        if key_findings.get("conflict_points"):
            yield {
                "type": "reasoning",
                "agent": "search",
                "step": "å†²çªè¯†åˆ«",
                "content": f"âš ï¸ å‘ç°ä¿¡æ¯å†²çª:\n" +
                           "\n".join([f"   â€¢ {c}" for c in key_findings.get("conflict_points", [])])
            }

        # æ•´ç†ç»“æœ
        unique_sources = self._deduplicate_sources(analyzed_sources)
        ranked_sources = self._rank_sources_by_importance(unique_sources, key_findings)

        key_sources = [s for s in ranked_sources if s.get("is_key_source", False)][:8]
        regular_sources = [s for s in ranked_sources if not s.get("is_key_source", False)][:12]

        yield {
            "type": "reasoning",
            "agent": "search",
            "step": "åˆ†ææ€»ç»“",
            "content": f"âœ… åˆ†æå®Œæˆ!\n"
                       f"   ğŸ“Œ å…³é”®ä¿¡æº: {len(key_sources)} ä¸ª\n"
                       f"   ğŸ“„ æ™®é€šä¿¡æº: {len(regular_sources)} ä¸ª\n"
                       f"   ğŸ” æ€»è¦†ç›–ç‡: {min(95, 50 + len(unique_sources) * 3)}%"
        }

        # æœ€ç»ˆç»“æœ
        result = {
            "search_id": search_id,
            "parser_task_ref": parser_result.get("task_id"),
            "original_query": original_content,
            "query_analysis": query_analysis,
            "key_sources": key_sources,
            "regular_sources": regular_sources,
            "all_sources": ranked_sources[:20],
            "analysis": {
                "search_reasoning_chain": query_reasoning,
                "key_findings": key_findings.get("findings", []),
                "conflict_points": key_findings.get("conflict_points", []),
                "evidence_gaps": key_findings.get("evidence_gaps", []),
                "analysis_reasoning": key_findings.get("analysis_reasoning", ""),
                "perspectives": key_findings.get("perspectives", {})
            },
            "search_metadata": {
                "total_queries": len(search_queries),
                "executed_queries": min(len(search_queries), 4),
                "sources_found": len(all_sources),
                "sources_after_dedup": len(unique_sources),
                "key_sources_count": len(key_sources),
                "coverage_score": min(0.95, 0.5 + len(unique_sources) * 0.03),
                "analysis_depth": "deep",
                "search_duration_ms": 8000
            }
        }

        yield {
            "type": "result",
            "agent": "search",
            "data": result
        }

    async def _execute_web_search(self, query: str, original_content: str, query_analysis: Dict) -> Dict[str, Any]:
        """
        ä½¿ç”¨ DeepSeek è”ç½‘åŠŸèƒ½æ‰§è¡Œå•æ¬¡æœç´¢ï¼Œå¸¦ç€å¯¹é—®é¢˜çš„ç†è§£å»æœç´¢
        """
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿¡æ¯åˆ†æå¸ˆå’Œè°ƒæŸ¥è®°è€…ã€‚è¯·ä½¿ç”¨è”ç½‘æœç´¢åŠŸèƒ½ï¼Œé’ˆå¯¹ä»¥ä¸‹æŸ¥è¯¢è¿›è¡Œæ·±åº¦æœç´¢ã€‚

ã€åŸå§‹é—®é¢˜ã€‘
{original_content}

ã€å½“å‰æœç´¢ç­–ç•¥ã€‘
{query}

ã€é—®é¢˜èƒŒæ™¯åˆ†æã€‘
- æ ¸å¿ƒå®ä½“: {', '.join(query_analysis.get('core_entities', []))}
- æ ¸å¿ƒé—®é¢˜: {query_analysis.get('core_question', '')}
- æŸ¥è¯¢æ„å›¾: {query_analysis.get('query_intent', '')}
- éœ€è¦äº¤å‰éªŒè¯: {'æ˜¯' if query_analysis.get('need_cross_validation') else 'å¦'}

ã€ä½ çš„ä»»åŠ¡ã€‘
1. æ‰§è¡Œè”ç½‘æœç´¢ï¼Œæ”¶é›†ç›¸å…³ä¿¡æº
2. åˆ†ææ¯ä¸ªä¿¡æºçš„å¯ä¿¡åº¦ã€ç«‹åœºå’Œæ½œåœ¨åè§
3. è¯„ä¼°ä¿¡æºä¸é—®é¢˜çš„ç›¸å…³æ€§å’Œé‡è¦æ€§
4. æ€è€ƒè¿™ä¸ªä¿¡æºå¯èƒ½å¦‚ä½•å¸®åŠ©å›ç­”é—®é¢˜

è¯·è¿”å›ä»¥ä¸‹æ ¼å¼çš„ç»“æœï¼ˆJSONï¼‰ï¼š
{{
    "search_reasoning": "è¯¦ç»†çš„æœç´¢æ€è·¯å’Œåˆ†æè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä½ å¦‚ä½•é€‰æ‹©å…³é”®è¯ã€å¦‚ä½•è¯„ä¼°ä¿¡æºä»·å€¼",
    "sources": [
        {{
            "evidence_id": "å”¯ä¸€ID",
            "title": "æ–‡ç« æ ‡é¢˜",
            "source_url": "https://...",
            "source_domain": "ç½‘ç«™åŸŸå",
            "publish_time": "å‘å¸ƒæ—¶é—´",
            "content_snippet": "å†…å®¹æ‘˜è¦ï¼ˆ200å­—ä»¥å†…ï¼Œçªå‡ºä¸é—®é¢˜çš„å…³è”ï¼‰",
            "source_credibility": "high|medium|low",
            "credibility_reason": "å¯ä¿¡åº¦è¯„ä¼°çš„è¯¦ç»†ç†ç”±",
            "source_category": "news|government|academic|social|blog",
            "source_stance": "neutral|supportive|opposing|unclear",
            "potential_bias": "æ½œåœ¨åè§è¯´æ˜ï¼ˆå¦‚æœ‰ï¼‰",
            "relevance_score": 0.95,
            "evidence_type": "primary|secondary",
            "key_insight": "è¿™ä¸ªä¿¡æºæä¾›çš„å…³é”®ä¿¡æ¯æˆ–è§‚ç‚¹",
            "importance_note": "ä¸ºä»€ä¹ˆè¿™ä¸ªä¿¡æºé‡è¦"
        }}
    ]
}}

è¦æ±‚ï¼š
1. è¿”å›4-10ä¸ªæœ€ç›¸å…³ã€æœ€æœ‰ä»·å€¼çš„ä¿¡æº
2. ä¼˜å…ˆå®˜æ–¹åª’ä½“ã€æ”¿åºœç½‘ç«™ã€å­¦æœ¯æœºæ„ã€æƒå¨åª’ä½“
3. æ³¨æ„è¯†åˆ«å¯èƒ½çš„åè§ä¿¡æºï¼Œå¹¶æ ‡æ³¨
4. æ¯ä¸ªä¿¡æºå¿…é¡»åŒ…å«çœŸå®å¯è®¿é—®çš„URL
5. è¯¦ç»†è¯´æ˜æœç´¢æ€è·¯å’Œä½ å‘ç°çš„å…³é”®ä¿¡æ¯
6. æ€è€ƒä¸åŒç«‹åœºçš„ä¿¡æºï¼Œç¡®ä¿è§‚ç‚¹å¤šå…ƒ"""

        result_text = await self._call_llm_with_search(prompt)
        return self._parse_search_result(result_text)

    async def _analyze_sources_deep(self, sources: List[Dict], original_content: str, query_analysis: Dict) -> List[Dict]:
        """
        ç²¾é€‰4-5ç¯‡æœ€é‡è¦ä¿¡æºè¿›è¡Œç²¾è¯»åˆ†æï¼Œèšç„¦æ ¸å¿ƒé—®é¢˜
        """
        if not sources:
            return []

        # ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿç­›é€‰å‡ºæœ€æœ‰ä»·å€¼çš„å€™é€‰ä¿¡æºï¼ˆæœ€å¤š8ä¸ªï¼‰
        candidates = self._select_candidate_sources(sources, query_analysis)
        
        # å‡†å¤‡å€™é€‰ä¿¡æºæ‘˜è¦
        sources_summary = []
        for i, s in enumerate(candidates):
            sources_summary.append({
                "index": i,
                "domain": s.get("source_domain", ""),
                "title": s.get("title", "")[:80],
                "credibility": s.get("source_credibility", "medium"),
                "stance": s.get("source_stance", "neutral"),
                "insight": s.get("key_insight", "")[:150],
                "relevance_score": s.get("relevance_score", 0.5)
            })

        prompt = f"""ä½ æ˜¯ä¸€ä½é«˜æ•ˆçš„ä¿¡æ¯åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼šä»å€™é€‰ä¿¡æºä¸­ç²¾é€‰4-5ç¯‡æœ€é‡è¦çš„è¿›è¡Œç²¾è¯»ï¼Œç›´æ¥æœåŠ¡äºæ ¸å¿ƒé—®é¢˜çš„éªŒè¯ã€‚

ã€æ ¸å¿ƒä»»åŠ¡ã€‘
{original_content}

ã€æ ¸å¿ƒé—®é¢˜ã€‘
{query_analysis.get('core_question', '')}

ã€éœ€è¦éªŒè¯çš„å…³é”®ç‚¹ã€‘
{', '.join(query_analysis.get('core_entities', []))}

ã€å€™é€‰ä¿¡æºã€‘
{json.dumps(sources_summary, ensure_ascii=False, indent=2)}

ã€ä½ çš„åˆ†æåŸåˆ™ã€‘
1. **èšç„¦ç›®æ ‡**ï¼šæ—¶åˆ»è®°å¾—ä½ è¦è¯æ˜/è¯ä¼ªä»€ä¹ˆï¼Œä¸è¦ä¸ºäº†åˆ†æè€Œåˆ†æ
2. **ç²¾é€‰ç²¾è¯»**ï¼šåªé€‰4-5ç¯‡æœ€èƒ½å¸®åŠ©å›ç­”æ ¸å¿ƒé—®é¢˜çš„ä¿¡æºæ·±å…¥åˆ†æ
3. **ç›´æ¥ç›¸å…³**ï¼šä¼˜å…ˆé€‰æ‹©ä¸æ ¸å¿ƒé—®é¢˜ç›´æ¥ç›¸å…³ã€èƒ½æä¾›å…³é”®è¯æ®çš„ä¿¡æº
4. **å¤šå…ƒè§†è§’**ï¼šç¡®ä¿é€‰å–çš„ä¿¡æºè¦†ç›–ä¸åŒç«‹åœºï¼ˆæ”¯æŒ/åå¯¹/ä¸­ç«‹ï¼‰
5. **è¯æ®ä»·å€¼**ï¼šè¯„ä¼°æ¯ä¸ªä¿¡æºå¯¹æ ¸å¿ƒé—®é¢˜çš„è¯æ˜/è¯ä¼ªä»·å€¼

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "selected_indices": [0, 2, 3, 5, 7],
    "selection_reasoning": "ä¸ºä»€ä¹ˆé€‰æ‹©è¿™4-5ç¯‡ä¿¡æºï¼Œå®ƒä»¬å¦‚ä½•å¸®åŠ©å›ç­”æ ¸å¿ƒé—®é¢˜",
    "source_analysis": [
        {{
            "index": 0,
            "analysis": "ç²¾è¯»åˆ†æï¼šè¿™ä¸ªä¿¡æºå¯¹æ ¸å¿ƒé—®é¢˜çš„ç›´æ¥è´¡çŒ®",
            "key_evidence": "è¯¥ä¿¡æºæä¾›çš„å…³é”®è¯æ®ï¼ˆç›´æ¥å¼•ç”¨æˆ–æ€»ç»“ï¼‰",
            "evidence_value": "high|medium|low - å¯¹è¯æ˜/è¯ä¼ªæ ¸å¿ƒé—®é¢˜çš„ä»·å€¼",
            "reliability_concerns": "å¯é æ€§æ–¹é¢çš„æ‹…å¿§ï¼ˆå¦‚æœ‰ï¼‰"
        }}
    ],
    "evidence_summary": "ç»¼åˆè¿™4-5ç¯‡ä¿¡æºï¼Œå¯¹æ ¸å¿ƒé—®é¢˜çš„è¯æ®æ”¯æŒæƒ…å†µæ€»ç»“"
}}

æ³¨æ„ï¼šåªåˆ†æ4-5ç¯‡æœ€é‡è¦çš„ä¿¡æºï¼Œä¸è¦é¢é¢ä¿±åˆ°ã€‚"""

        try:
            result_text = await self._call_llm_with_search(prompt)
            analysis_result = self._parse_search_result(result_text)
            
            # å°†åˆ†æç»“æœåˆå¹¶åˆ°åŸä¿¡æºï¼ˆåªé’ˆå¯¹è¢«é€‰ä¸­çš„ä¿¡æºï¼‰
            selected_indices = analysis_result.get("selected_indices", [])
            source_analysis = analysis_result.get("source_analysis", [])
            
            # æ ‡è®°è¢«é€‰ä¸­çš„ä¿¡æº
            for idx in selected_indices:
                if idx < len(candidates):
                    original_idx = candidates[idx].get("_original_index", idx)
                    if original_idx < len(sources):
                        sources[original_idx]["is_deep_analyzed"] = True
                        sources[original_idx]["selection_reasoning"] = analysis_result.get("selection_reasoning", "")
            
            # åˆå¹¶æ·±åº¦åˆ†æç»“æœ
            for analysis in source_analysis:
                idx = analysis.get("index", 0)
                if idx < len(candidates):
                    original_idx = candidates[idx].get("_original_index", idx)
                    if original_idx < len(sources):
                        sources[original_idx]["deep_analysis"] = analysis.get("analysis", "")
                        sources[original_idx]["key_evidence"] = analysis.get("key_evidence", "")
                        sources[original_idx]["evidence_value"] = analysis.get("evidence_value", "medium")
                        sources[original_idx]["reliability_concerns"] = analysis.get("reliability_concerns", "")

            print(f"[SearchAgent] Deep analyzed {len(selected_indices)} key sources out of {len(sources)} total")
            return sources
        except Exception as e:
            print(f"[SearchAgent] Deep analysis error: {e}")
            return sources

    def _select_candidate_sources(self, sources: List[Dict], query_analysis: Dict) -> List[Dict]:
        """
        å¿«é€Ÿç­›é€‰æœ€æœ‰ä»·å€¼çš„å€™é€‰ä¿¡æºï¼ˆæœ€å¤š8ä¸ªï¼‰ä¾›ç²¾è¯»
        """
        if not sources:
            return []
        
        # è®¡ç®—æ¯ä¸ªä¿¡æºçš„åˆå§‹åˆ†æ•°
        scored_sources = []
        for i, source in enumerate(sources):
            score = 0
            
            # å¯ä¿¡åº¦æƒé‡ï¼ˆé«˜å¯ä¿¡åº¦åŠ åˆ†ï¼‰
            credibility_scores = {"high": 3, "medium": 2, "low": 1}
            score += credibility_scores.get(source.get("source_credibility", "low"), 1) * 5
            
            # ç›¸å…³åº¦æƒé‡
            score += source.get("relevance_score", 0.5) * 10
            
            # è¯æ®ç±»å‹ï¼ˆä¸€æ‰‹è¯æ®åŠ åˆ†ï¼‰
            if source.get("evidence_type") == "primary":
                score += 5
            
            # æ˜¯å¦æœ‰ç‹¬ç‰¹è§è§£
            if source.get("key_insight") and len(source.get("key_insight", "")) > 20:
                score += 3
            
            source["_original_index"] = i
            source["_candidate_score"] = score
            scored_sources.append(source)
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œå–å‰8ä¸ª
        scored_sources.sort(key=lambda x: -x.get("_candidate_score", 0))
        return scored_sources[:8]

    async def _identify_key_findings(self, sources: List[Dict], original_content: str, query_analysis: Dict) -> Dict[str, Any]:
        """
        è¯†åˆ«å…³é”®å‘ç°ã€å†²çªç‚¹å’Œè¯æ®ç¼ºå£ - èšç„¦æ ¸å¿ƒé—®é¢˜
        """
        if not sources:
            return {
                "findings": [],
                "conflict_points": [],
                "evidence_gaps": ["æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³ä¿¡æº"],
                "analysis_reasoning": "",
                "perspectives": {},
                "key_source_indices": []
            }

        # ä¼˜å…ˆä½¿ç”¨å·²æ·±åº¦åˆ†æçš„ä¿¡æºï¼Œå¦‚æœæ²¡æœ‰åˆ™é€‰é«˜å¯ä¿¡åº¦ä¿¡æº
        deep_analyzed = [s for s in sources if s.get("is_deep_analyzed")]
        if deep_analyzed:
            key_sources = deep_analyzed[:6]
        else:
            # æŒ‰å¯ä¿¡åº¦æ’åºå–å‰8ä¸ª
            key_sources = sorted(
                sources, 
                key=lambda x: {"high": 3, "medium": 2, "low": 1}.get(x.get("source_credibility", "low"), 0),
                reverse=True
            )[:8]

        # å‡†å¤‡å…³é”®ä¿¡æ¯æ‘˜è¦
        key_info = []
        key_indices = []
        for i, s in enumerate(sources):
            if s in key_sources:
                key_indices.append(i)
                key_info.append({
                    "index": i,
                    "domain": s.get("source_domain", ""),
                    "insight": s.get("key_insight", "")[:150],
                    "stance": s.get("source_stance", "neutral"),
                    "credibility": s.get("source_credibility", "medium"),
                    "is_deep_analyzed": s.get("is_deep_analyzed", False),
                    "evidence_value": s.get("evidence_value", "medium")
                })

        prompt = f"""ä½ æ˜¯ä¸€ä½ç›®æ ‡å¯¼å‘çš„äº‹å®æ ¸æŸ¥ä¸“å®¶ã€‚åŸºäºç²¾é€‰ä¿¡æºï¼Œç›´æ¥å›ç­”æ ¸å¿ƒé—®é¢˜ã€‚

ã€æ ¸å¿ƒä»»åŠ¡ - æ—¶åˆ»ç‰¢è®°ã€‘
{original_content}

ã€æ ¸å¿ƒé—®é¢˜ã€‘
{query_analysis.get('core_question', '')}

ã€éœ€è¦éªŒè¯çš„å…³é”®ç‚¹ã€‘
{', '.join(query_analysis.get('core_entities', []))}

ã€ç²¾é€‰ä¿¡æºï¼ˆå·²ç­›é€‰å‡ºæœ€é‡è¦çš„ï¼‰ã€‘
{json.dumps(key_info, ensure_ascii=False, indent=2)}

ã€ä½ çš„åˆ†æåŸåˆ™ã€‘
1. **ç›®æ ‡å¯¼å‘**ï¼šæ‰€æœ‰åˆ†æéƒ½æœåŠ¡äºå›ç­”æ ¸å¿ƒé—®é¢˜ï¼Œä¸è¦å‘æ•£
2. **è¯æ®è¯´è¯**ï¼šåŸºäºä¿¡æºä¸­çš„å…·ä½“è¯æ®ï¼Œè€Œéæ³›æ³›è€Œè°ˆ
3. **èšç„¦å…³é”®**ï¼šåªå…³æ³¨å¯¹æ ¸å¿ƒé—®é¢˜æœ‰ç›´æ¥å½±å“çš„ä¿¡æ¯
4. **å¿«é€Ÿåˆ¤æ–­**ï¼šç»™å‡ºæ˜ç¡®çš„å€¾å‘æ€§åˆ¤æ–­ï¼Œè€Œéæ¨¡æ£±ä¸¤å¯

ã€ä½ çš„åˆ†æä»»åŠ¡ã€‘
1. **æ ¸å¿ƒå‘ç°**ï¼ˆ2-4æ¡ï¼‰ï¼šç›´æ¥å›ç­”æ ¸å¿ƒé—®é¢˜çš„å…³é”®äº‹å®
2. **ä¿¡æ¯å†²çª**ï¼ˆå¦‚æœ‰ï¼‰ï¼šå½±å“åˆ¤æ–­çš„å…³é”®çŸ›ç›¾ç‚¹
3. **è¯æ®ç¼ºå£**ï¼šè¿˜ç¼ºå°‘ä»€ä¹ˆå…³é”®ä¿¡æ¯æ‰èƒ½ä¸‹å®šè®º
4. **åˆæ­¥åˆ¤æ–­**ï¼šåŸºäºç°æœ‰è¯æ®ï¼Œæ ¸å¿ƒé—®é¢˜çš„ç­”æ¡ˆå€¾å‘

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "findings": [
        "æ ¸å¿ƒå‘ç°1ï¼šç›´æ¥å…³è”æ ¸å¿ƒé—®é¢˜çš„å…³é”®äº‹å®",
        "æ ¸å¿ƒå‘ç°2ï¼š..."
    ],
    "conflict_points": [
        "å…³é”®å†²çªï¼šç›´æ¥å½±å“åˆ¤æ–­çš„çŸ›ç›¾ç‚¹"
    ],
    "evidence_gaps": [
        "ç¼ºå¤±çš„å…³é”®è¯æ®"
    ],
    "analysis_reasoning": "ç®€æ´çš„æ¨ç†è¿‡ç¨‹ï¼šåŸºäºä»€ä¹ˆè¯æ®ï¼Œå¾—å‡ºä»€ä¹ˆåˆæ­¥åˆ¤æ–­",
    "perspectives": {{
        "supporting": "æ”¯æŒæ–¹çš„æ ¸å¿ƒè®ºæ®ï¼ˆä¸€å¥è¯æ€»ç»“ï¼‰",
        "opposing": "åå¯¹æ–¹çš„æ ¸å¿ƒè®ºæ®ï¼ˆä¸€å¥è¯æ€»ç»“ï¼‰"
    }},
    "key_source_indices": {key_indices[:5]}
}}

æ³¨æ„ï¼šä¿æŒç®€æ´èšç„¦ï¼Œä¸è¦è¿‡åº¦åˆ†æã€‚"""

        try:
            result_text = await self._call_llm_with_search(prompt)
            return self._parse_search_result(result_text)
        except Exception as e:
            print(f"[SearchAgent] Key findings error: {e}")
            return {
                "findings": ["åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯"],
                "conflict_points": [],
                "evidence_gaps": [],
                "analysis_reasoning": f"é”™è¯¯: {str(e)}",
                "perspectives": {},
                "key_source_indices": []
            }

    async def _call_llm_with_search(self, prompt: str) -> str:
        """è°ƒç”¨æ”¯æŒè”ç½‘åŠŸèƒ½çš„ LLM (DeepSeek via é˜¿é‡Œç™¾ç‚¼)"""
        try:
            if self.llm_provider == "openai" and self.openai_client:
                print(f"[SearchAgent] Calling DeepSeek with web search...")
                
                # é˜¿é‡Œç™¾ç‚¼ DeepSeek è”ç½‘æœç´¢é…ç½®
                # å‚è€ƒ: https://help.aliyun.com/zh/model-studio/user-guide/deepseek
                response = await self.openai_client.chat.completions.create(
                    model=settings.OPENAI_MODEL,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿¡æ¯åˆ†æå¸ˆã€è°ƒæŸ¥è®°è€…å’Œäº‹å®æ ¸æŸ¥ä¸“å®¶ã€‚ä½ æ“…é•¿æ·±åº¦æœç´¢ã€æ‰¹åˆ¤æ€§æ€ç»´å’Œå¤šè§’åº¦åˆ†æã€‚ä½ æ€»æ˜¯åŸºäºè¯æ®è¯´è¯ï¼Œå–„äºå‘ç°ä¿¡æ¯å†²çªå’Œåè§ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.4,
                    max_tokens=4000,
                    # é˜¿é‡Œç™¾ç‚¼è”ç½‘æœç´¢é…ç½®
                    # ä½¿ç”¨ enable_search å‚æ•°å¯ç”¨è”ç½‘æœç´¢ï¼ˆé˜¿é‡Œç™¾ç‚¼ç‰¹å®šå‚æ•°ï¼‰
                    extra_body={
                        "enable_search": True
                    }
                )
                
                content = response.choices[0].message.content
                print(f"[SearchAgent] LLM Response: {content[:200]}...")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ç»“æœï¼ˆæœç´¢ç»“æœï¼‰
                if hasattr(response.choices[0], 'tool_calls') and response.choices[0].tool_calls:
                    print(f"[SearchAgent] Web search tool was used")
                
                return content
            elif self.llm_provider == "claude" and self.anthropic_client:
                # Claude ç›®å‰ä¸ç›´æ¥æ”¯æŒè”ç½‘æœç´¢ï¼Œéœ€è¦é…åˆå…¶ä»–æœç´¢å·¥å…·
                print(f"[SearchAgent] Claude does not support web search directly")
                response = self.anthropic_client.messages.create(
                    model=settings.ANTHROPIC_MODEL,
                    max_tokens=4000,
                    temperature=0.4,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text
            else:
                return "{}"
        except Exception as e:
            print(f"[SearchAgent] LLM Error: {str(e)}")
            return "{}"

    def _parse_search_result(self, result_text: str) -> Dict[str, Any]:
        """è§£æ LLM è¿”å›çš„æœç´¢ç»“æœ"""
        if not result_text or not result_text.strip():
            return {"sources": [], "search_reasoning": "", "findings": [], "conflict_points": [], "evidence_gaps": []}

        text = result_text.strip()

        # æ¸…ç† markdown
        if text.startswith("```json"):
            text = text[7:]
        elif text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        text = text.strip()

        try:
            result = json.loads(text)
            
            # ä¸ºæ¯ä¸ªä¿¡æºæ·»åŠ  ID å’Œé»˜è®¤å€¼
            if "sources" in result:
                for source in result["sources"]:
                    if not source.get("evidence_id"):
                        source["evidence_id"] = str(uuid.uuid4())
                    if not source.get("source_category"):
                        source["source_category"] = "news"
                    if not source.get("relevance_score"):
                        source["relevance_score"] = 0.8
                    if not source.get("evidence_type"):
                        source["evidence_type"] = "primary"
                    if not source.get("source_stance"):
                        source["source_stance"] = "neutral"
            
            return result
        except json.JSONDecodeError:
            # å°è¯•æå– JSON
            try:
                start = text.find("{")
                end = text.rfind("}") + 1
                if start >= 0 and end > start:
                    return json.loads(text[start:end])
            except:
                pass
            return {"sources": [], "search_reasoning": ""}

    def _deduplicate_sources(self, sources: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æŒ‰ URL å»é‡ï¼Œä¿ç•™æœ€å®Œæ•´çš„ç‰ˆæœ¬"""
        seen_urls = {}
        
        for source in sources:
            url = source.get("source_url", "")
            if url:
                if url not in seen_urls:
                    seen_urls[url] = source
                else:
                    # ä¿ç•™æ›´å®Œæ•´çš„ç‰ˆæœ¬ï¼ˆæœ‰æ›´å¤šå­—æ®µçš„ï¼‰
                    existing = seen_urls[url]
                    if len(str(source)) > len(str(existing)):
                        seen_urls[url] = source
        
        return list(seen_urls.values())

    def _rank_sources_by_importance(self, sources: List[Dict[str, Any]], key_findings: Dict) -> List[Dict[str, Any]]:
        """
        æŒ‰é‡è¦æ€§æ’åºä¿¡æºï¼Œä¼˜å…ˆå±•ç¤ºè¢«æ·±åº¦åˆ†æçš„å…³é”®ä¿¡æº
        """
        key_indices = set(key_findings.get("key_source_indices", []))
        
        # ä¸ºæ¯ä¸ªä¿¡æºè®¡ç®—é‡è¦æ€§åˆ†æ•°
        for i, source in enumerate(sources):
            score = 0
            
            # æ˜¯å¦è¢«æ·±åº¦åˆ†æï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            if source.get("is_deep_analyzed"):
                score += 50
                source["is_key_source"] = True
            # æ˜¯å¦è¢«æ ‡è®°ä¸ºå…³é”®ä¿¡æº
            elif i in key_indices:
                score += 30
                source["is_key_source"] = True
            else:
                source["is_key_source"] = False
            
            # è¯æ®ä»·å€¼æƒé‡
            evidence_value_scores = {"high": 15, "medium": 8, "low": 3}
            score += evidence_value_scores.get(source.get("evidence_value", "medium"), 5)
            
            # å¯ä¿¡åº¦æƒé‡
            credibility_scores = {"high": 10, "medium": 5, "low": 2}
            score += credibility_scores.get(source.get("source_credibility", "low"), 2)
            
            # ç›¸å…³åº¦æƒé‡
            score += source.get("relevance_score", 0.5) * 10
            
            # æ˜¯å¦æœ‰ä¸€æ‰‹è¯æ®
            if source.get("evidence_type") == "primary":
                score += 8
            
            source["importance_score"] = score
        
        # æŒ‰é‡è¦æ€§åˆ†æ•°æ’åº
        return sorted(sources, key=lambda x: -x.get("importance_score", 0))
