import json
import uuid
from typing import List, Dict, Any, AsyncGenerator
import openai
from anthropic import Anthropic
import asyncio

from app.core.config import settings


class SearchAgent:
    """
    æœç´¢åˆ†æ Agent - ä¸“ä¸šçš„ä¿¡æ¯åˆ†æå¸ˆå’Œæ‰¾èŒ¬ä¸“å®¶
    
    èŒè´£ï¼š
    1. æ·±åº¦ç†è§£é—®é¢˜çš„æ ¸å¿ƒçŸ›ç›¾å’Œå…³é”®ç‚¹
    2. æ‰§è¡Œç²¾å‡†æœç´¢ï¼Œæ”¶é›†å¤šæ–¹è¯æ®
    3. åˆ†æä¿¡æºçš„å¯ä¿¡åº¦ã€ç«‹åœºå’Œæ½œåœ¨åè§
    4. è¯†åˆ«ä¿¡æ¯å†²çªç‚¹å’Œçªç ´å£
    5. ç­›é€‰æœ€å…³é”®çš„ä¿¡æºï¼Œæ ‡è®°é‡è¦è¯æ®
    """

    def __init__(self):
        self.llm_provider = settings.LLM_PROVIDER
        self.openai_client = openai.AsyncOpenAI(
            api_key=settings.OPENAI_API_KEY,
            base_url=settings.OPENAI_BASE_URL,
            timeout=120.0
        ) if settings.OPENAI_API_KEY else None
        self.anthropic_client = Anthropic(
            api_key=settings.ANTHROPIC_API_KEY
        ) if settings.ANTHROPIC_API_KEY else None
        print(f"[SearchAgent] Initialized with LLM provider: {self.llm_provider}")

    async def search(self, parser_result: Dict[str, Any], original_content: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ·±åº¦æœç´¢å’Œåˆ†æ

        Args:
            parser_result: Parser Agent çš„è¾“å‡ºï¼ŒåŒ…å«æœç´¢æŸ¥è¯¢åˆ—è¡¨å’Œåˆ†æ
            original_content: ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢å†…å®¹

        Returns:
            åŒ…å«æ·±åº¦åˆ†æçš„ä¿¡æºæ•°æ®é›†
        """
        search_id = str(uuid.uuid4())
        search_queries = parser_result.get("search_queries", [])
        query_analysis = parser_result.get("analysis", {})

        print(f"[SearchAgent] Starting deep analysis with {len(search_queries)} queries")

        # é˜¶æ®µ1: æ‰§è¡Œå¤šæ¬¡æœç´¢æ”¶é›†è¯æ®
        all_sources = []
        query_reasoning = []

        for i, query in enumerate(search_queries[:4]):
            print(f"[SearchAgent] Query {i+1}/{len(search_queries)}: {query}")

            result = await self._execute_web_search(query, original_content, query_analysis)
            sources = result.get("sources", [])
            reasoning = result.get("search_reasoning", "")

            all_sources.extend(sources)
            if reasoning:
                query_reasoning.append({
                    "query": query,
                    "reasoning": reasoning
                })

            print(f"[SearchAgent] Query {i+1} returned {len(sources)} sources")

        # é˜¶æ®µ2: æ·±åº¦åˆ†æä¿¡æº
        print(f"[SearchAgent] Starting source analysis...")
        analyzed_sources = await self._analyze_sources_deep(
            all_sources, original_content, query_analysis
        )

        # é˜¶æ®µ3: è¯†åˆ«å…³é”®è¯æ®å’Œçªç ´å£
        print(f"[SearchAgent] Identifying key evidence...")
        key_findings = await self._identify_key_findings(
            analyzed_sources, original_content, query_analysis
        )

        # é˜¶æ®µ4: æ•´ç†è¾“å‡º
        unique_sources = self._deduplicate_sources(analyzed_sources)
        ranked_sources = self._rank_sources_by_importance(unique_sources, key_findings)

        # åˆ†ç¦»å…³é”®ä¿¡æºå’Œæ™®é€šä¿¡æº
        key_sources = [s for s in ranked_sources if s.get("is_key_source", False)][:8]
        regular_sources = [s for s in ranked_sources if not s.get("is_key_source", False)][:12]

        print(f"[SearchAgent] Analysis complete: {len(key_sources)} key sources, {len(regular_sources)} regular sources")

        return {
            "search_id": search_id,
            "parser_task_ref": parser_result.get("task_id"),
            "original_query": original_content,
            "query_analysis": query_analysis,
            
            # å…³é”®ä¿¡æºï¼ˆæœ€é‡è¦çš„çªç ´å£ï¼‰
            "key_sources": key_sources,
            
            # æ™®é€šä¿¡æº
            "regular_sources": regular_sources,
            
            # æ‰€æœ‰ä¿¡æºï¼ˆåˆå¹¶ï¼‰
            "all_sources": ranked_sources[:20],
            
            # æ·±åº¦åˆ†æç»“æœ
            "analysis": {
                # æœç´¢è¿‡ç¨‹æ¨ç†
                "search_reasoning_chain": query_reasoning,
                
                # æ ¸å¿ƒå‘ç°
                "key_findings": key_findings.get("findings", []),
                
                # ä¿¡æ¯å†²çªç‚¹
                "conflict_points": key_findings.get("conflict_points", []),
                
                # è¯æ®ç¼ºå£
                "evidence_gaps": key_findings.get("evidence_gaps", []),
                
                # åˆ†ææ¨ç†è¿‡ç¨‹
                "analysis_reasoning": key_findings.get("analysis_reasoning", ""),
                
                # å¤šè§’åº¦è§‚ç‚¹æ±‡æ€»
                "perspectives": key_findings.get("perspectives", {})
            },
            
            # å…ƒæ•°æ®
            "search_metadata": {
                "total_queries": len(search_queries),
                "executed_queries": min(len(search_queries), 4),
                "sources_found": len(all_sources),
                "sources_after_dedup": len(unique_sources),
                "key_sources_count": len(key_sources),
                "coverage_score": min(0.95, 0.5 + len(unique_sources) * 0.03),
                "analysis_depth": "deep",
                "search_duration_ms": 8000
            }
        }

    async def search_stream(self, parser_result: Dict[str, Any], original_content: str) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼æœç´¢åˆ†æï¼Œå®æ—¶è¿”å›æ¨ç†è¿‡ç¨‹
        """
        search_id = str(uuid.uuid4())
        search_queries = parser_result.get("search_queries", [])
        query_analysis = parser_result.get("analysis", {})

        # å¼€å§‹åˆ†æ - è¯¦ç»†æ¨ç†è¿‡ç¨‹
        core_entities = query_analysis.get('core_entities', [])
        core_question = query_analysis.get('core_question', '')
        info_types = query_analysis.get('info_types', [])

        yield {
            "type": "reasoning",
            "agent": "search",
            "step": "é—®é¢˜ç†è§£",
            "content": f"ğŸ§  Search Agent å¼€å§‹æ·±åº¦åˆ†æ...\n\n"
                       f"ğŸ“Œ æ ¸å¿ƒå®ä½“è¯†åˆ«:\n" +
                       chr(10).join([f"   â€¢ {e}" for e in core_entities]) +
                       f"\n\nğŸ¯ æ ¸å¿ƒé—®é¢˜:\n   {core_question}\n\n"
                       f"ğŸ“‹ ä¿¡æ¯ç±»å‹: {', '.join(info_types)}\n\n"
                       f"ğŸ’­ åˆ†ææ€è·¯:\n"
                       f"   1. ç†è§£é—®é¢˜çš„æ ¸å¿ƒçŸ›ç›¾å’Œå…³é”®ç‚¹\n"
                       f"   2. è®¾è®¡ç²¾å‡†æœç´¢ç­–ç•¥ï¼Œè¦†ç›–å¤šè§’åº¦\n"
                       f"   3. è¯„ä¼°æ¯ä¸ªä¿¡æºçš„å¯ä¿¡åº¦å’Œç«‹åœº\n"
                       f"   4. è¯†åˆ«ä¿¡æ¯å†²çªå’Œçªç ´å£"
        }

        all_sources = []
        query_reasoning = []

        # æ‰§è¡Œå¤šæ¬¡æœç´¢
        for i, query in enumerate(search_queries[:4]):
            yield {
                "type": "reasoning",
                "agent": "search",
                "step": f"æœç´¢{i+1}",
                "content": f"ğŸ” æ‰§è¡Œç¬¬ {i+1} è½®æœç´¢...\n\n"
                           f"ğŸ“¡ æœç´¢æŸ¥è¯¢:\n   {query}\n\n"
                           f"ğŸ’¡ æœç´¢ç­–ç•¥:\n"
                           f"   â€¢ ä½¿ç”¨è”ç½‘æœç´¢è·å–å®æ—¶ä¿¡æ¯\n"
                           f"   â€¢ ç­›é€‰é«˜å¯ä¿¡åº¦ä¿¡æº\n"
                           f"   â€¢ è®°å½•æœç´¢æ€è·¯å’Œå…³é”®å‘ç°"
            }

            result = await self._execute_web_search(query, original_content, query_analysis)
            sources = result.get("sources", [])
            reasoning = result.get("search_reasoning", "")

            all_sources.extend(sources)
            if reasoning:
                query_reasoning.append({
                    "query": query,
                    "reasoning": reasoning
                })

            yield {
                "type": "reasoning",
                "agent": "search",
                "step": f"æœç´¢{i+1}ç»“æœ",
                "content": f"âœ“ ç¬¬ {i+1} è½®æœç´¢å®Œæˆ\n"
                           f"   ğŸ“Š æ‰¾åˆ° {len(sources)} ä¸ªä¿¡æº\n"
                           f"   ğŸ’­ æœç´¢æ€è·¯: {reasoning}"
            }

            # æ˜¾ç¤ºæ¯ä¸ªä¿¡æºçš„è¯¦ç»†åˆ†æ
            for j, source in enumerate(sources, 1):
                credibility = source.get("source_credibility", "medium")
                credibility_emoji = "ğŸŸ¢" if credibility == "high" else "ğŸŸ¡" if credibility == "medium" else "ğŸ”´"
                credibility_text = "é«˜" if credibility == "high" else "ä¸­" if credibility == "medium" else "ä½"
                domain = source.get('source_domain', 'æœªçŸ¥')
                title = source.get('title', '')
                insight = source.get('key_insight', '')

                yield {
                    "type": "reasoning",
                    "agent": "search",
                    "step": f"ä¿¡æº{i+1}-{j}",
                    "content": f"   {credibility_emoji} ä¿¡æº {j}: [{credibility_text}å¯ä¿¡åº¦]\n"
                               f"      æ¥æº: {domain}\n"
                               f"      æ ‡é¢˜: {title}\n"
                               f"      å…³é”®ä¿¡æ¯: {insight}"
                }

            await asyncio.sleep(0.1)

        # æ·±åº¦åˆ†æé˜¶æ®µ
        yield {
            "type": "reasoning",
            "agent": "search",
            "step": "æ·±åº¦åˆ†æ",
            "content": f"ğŸ§  å¯¹ {len(all_sources)} ä¸ªä¿¡æºè¿›è¡Œæ·±åº¦åˆ†æ...\n"
                       f"   - è¯„ä¼°å¯ä¿¡åº¦å’Œç«‹åœº\n"
                       f"   - è¯†åˆ«ä¿¡æ¯å†²çªç‚¹\n"
                       f"   - å¯»æ‰¾å…³é”®çªç ´å£"
        }

        analyzed_sources = await self._analyze_sources_deep(
            all_sources, original_content, query_analysis
        )

        yield {
            "type": "reasoning",
            "agent": "search",
            "step": "ä¿¡æºè¯„ä¼°",
            "content": f"ğŸ“Š ä¿¡æºè¯„ä¼°å®Œæˆ:\n"
                       f"   - é«˜å¯ä¿¡åº¦: {sum(1 for s in analyzed_sources if s.get('source_credibility') == 'high')}\n"
                       f"   - ä¸­ç­‰å¯ä¿¡åº¦: {sum(1 for s in analyzed_sources if s.get('source_credibility') == 'medium')}\n"
                       f"   - å‘ç°åè§ä¿¡æº: {sum(1 for s in analyzed_sources if s.get('potential_bias'))}"
        }

        # è¯†åˆ«å…³é”®å‘ç°
        key_findings = await self._identify_key_findings(
            analyzed_sources, original_content, query_analysis
        )

        yield {
            "type": "reasoning",
            "agent": "search",
            "step": "å…³é”®å‘ç°",
            "content": f"ğŸ¯ å…³é”®å‘ç°:\n" +
                       "\n".join([f"   {i+1}. {f}" for i, f in enumerate(key_findings.get("findings", []))])
        }

        if key_findings.get("conflict_points"):
            yield {
                "type": "reasoning",
                "agent": "search",
                "step": "å†²çªè¯†åˆ«",
                "content": f"âš ï¸ å‘ç°ä¿¡æ¯å†²çª:\n" +
                           "\n".join([f"   â€¢ {c}" for c in key_findings.get("conflict_points", [])])
            }

        # æ•´ç†ç»“æœ
        unique_sources = self._deduplicate_sources(analyzed_sources)
        ranked_sources = self._rank_sources_by_importance(unique_sources, key_findings)

        key_sources = [s for s in ranked_sources if s.get("is_key_source", False)][:8]
        regular_sources = [s for s in ranked_sources if not s.get("is_key_source", False)][:12]

        yield {
            "type": "reasoning",
            "agent": "search",
            "step": "åˆ†ææ€»ç»“",
            "content": f"âœ… åˆ†æå®Œæˆ!\n"
                       f"   ğŸ“Œ å…³é”®ä¿¡æº: {len(key_sources)} ä¸ª\n"
                       f"   ğŸ“„ æ™®é€šä¿¡æº: {len(regular_sources)} ä¸ª\n"
                       f"   ğŸ” æ€»è¦†ç›–ç‡: {min(95, 50 + len(unique_sources) * 3)}%"
        }

        # æœ€ç»ˆç»“æœ
        result = {
            "search_id": search_id,
            "parser_task_ref": parser_result.get("task_id"),
            "original_query": original_content,
            "query_analysis": query_analysis,
            "key_sources": key_sources,
            "regular_sources": regular_sources,
            "all_sources": ranked_sources[:20],
            "analysis": {
                "search_reasoning_chain": query_reasoning,
                "key_findings": key_findings.get("findings", []),
                "conflict_points": key_findings.get("conflict_points", []),
                "evidence_gaps": key_findings.get("evidence_gaps", []),
                "analysis_reasoning": key_findings.get("analysis_reasoning", ""),
                "perspectives": key_findings.get("perspectives", {})
            },
            "search_metadata": {
                "total_queries": len(search_queries),
                "executed_queries": min(len(search_queries), 4),
                "sources_found": len(all_sources),
                "sources_after_dedup": len(unique_sources),
                "key_sources_count": len(key_sources),
                "coverage_score": min(0.95, 0.5 + len(unique_sources) * 0.03),
                "analysis_depth": "deep",
                "search_duration_ms": 8000
            }
        }

        yield {
            "type": "result",
            "agent": "search",
            "data": result
        }

    async def _execute_web_search(self, query: str, original_content: str, query_analysis: Dict) -> Dict[str, Any]:
        """
        ä½¿ç”¨ DeepSeek è”ç½‘åŠŸèƒ½æ‰§è¡Œå•æ¬¡æœç´¢ï¼Œå¸¦ç€å¯¹é—®é¢˜çš„ç†è§£å»æœç´¢
        """
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿¡æ¯åˆ†æå¸ˆå’Œè°ƒæŸ¥è®°è€…ã€‚è¯·ä½¿ç”¨è”ç½‘æœç´¢åŠŸèƒ½ï¼Œé’ˆå¯¹ä»¥ä¸‹æŸ¥è¯¢è¿›è¡Œæ·±åº¦æœç´¢ã€‚

ã€åŸå§‹é—®é¢˜ã€‘
{original_content}

ã€å½“å‰æœç´¢ç­–ç•¥ã€‘
{query}

ã€é—®é¢˜èƒŒæ™¯åˆ†æã€‘
- æ ¸å¿ƒå®ä½“: {', '.join(query_analysis.get('core_entities', []))}
- æ ¸å¿ƒé—®é¢˜: {query_analysis.get('core_question', '')}
- æŸ¥è¯¢æ„å›¾: {query_analysis.get('query_intent', '')}
- éœ€è¦äº¤å‰éªŒè¯: {'æ˜¯' if query_analysis.get('need_cross_validation') else 'å¦'}

ã€ä½ çš„ä»»åŠ¡ã€‘
1. æ‰§è¡Œè”ç½‘æœç´¢ï¼Œæ”¶é›†ç›¸å…³ä¿¡æº
2. åˆ†ææ¯ä¸ªä¿¡æºçš„å¯ä¿¡åº¦ã€ç«‹åœºå’Œæ½œåœ¨åè§
3. è¯„ä¼°ä¿¡æºä¸é—®é¢˜çš„ç›¸å…³æ€§å’Œé‡è¦æ€§
4. æ€è€ƒè¿™ä¸ªä¿¡æºå¯èƒ½å¦‚ä½•å¸®åŠ©å›ç­”é—®é¢˜

è¯·è¿”å›ä»¥ä¸‹æ ¼å¼çš„ç»“æœï¼ˆJSONï¼‰ï¼š
{{
    "search_reasoning": "è¯¦ç»†çš„æœç´¢æ€è·¯å’Œåˆ†æè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä½ å¦‚ä½•é€‰æ‹©å…³é”®è¯ã€å¦‚ä½•è¯„ä¼°ä¿¡æºä»·å€¼",
    "sources": [
        {{
            "evidence_id": "å”¯ä¸€ID",
            "title": "æ–‡ç« æ ‡é¢˜",
            "source_url": "https://...",
            "source_domain": "ç½‘ç«™åŸŸå",
            "publish_time": "å‘å¸ƒæ—¶é—´",
            "content_snippet": "å†…å®¹æ‘˜è¦ï¼ˆ200å­—ä»¥å†…ï¼Œçªå‡ºä¸é—®é¢˜çš„å…³è”ï¼‰",
            "source_credibility": "high|medium|low",
            "credibility_reason": "å¯ä¿¡åº¦è¯„ä¼°çš„è¯¦ç»†ç†ç”±",
            "source_category": "news|government|academic|social|blog",
            "source_stance": "neutral|supportive|opposing|unclear",
            "potential_bias": "æ½œåœ¨åè§è¯´æ˜ï¼ˆå¦‚æœ‰ï¼‰",
            "relevance_score": 0.95,
            "evidence_type": "primary|secondary",
            "key_insight": "è¿™ä¸ªä¿¡æºæä¾›çš„å…³é”®ä¿¡æ¯æˆ–è§‚ç‚¹",
            "importance_note": "ä¸ºä»€ä¹ˆè¿™ä¸ªä¿¡æºé‡è¦"
        }}
    ]
}}

è¦æ±‚ï¼š
1. è¿”å›4-10ä¸ªæœ€ç›¸å…³ã€æœ€æœ‰ä»·å€¼çš„ä¿¡æº
2. ä¼˜å…ˆå®˜æ–¹åª’ä½“ã€æ”¿åºœç½‘ç«™ã€å­¦æœ¯æœºæ„ã€æƒå¨åª’ä½“
3. æ³¨æ„è¯†åˆ«å¯èƒ½çš„åè§ä¿¡æºï¼Œå¹¶æ ‡æ³¨
4. æ¯ä¸ªä¿¡æºå¿…é¡»åŒ…å«çœŸå®å¯è®¿é—®çš„URL
5. è¯¦ç»†è¯´æ˜æœç´¢æ€è·¯å’Œä½ å‘ç°çš„å…³é”®ä¿¡æ¯
6. æ€è€ƒä¸åŒç«‹åœºçš„ä¿¡æºï¼Œç¡®ä¿è§‚ç‚¹å¤šå…ƒ"""

        result_text = await self._call_llm_with_search(prompt)
        return self._parse_search_result(result_text)

    async def _analyze_sources_deep(self, sources: List[Dict], original_content: str, query_analysis: Dict) -> List[Dict]:
        """
        å¯¹æ‰€æœ‰ä¿¡æºè¿›è¡Œæ·±åº¦åˆ†æï¼Œè¯†åˆ«æ¨¡å¼å’Œé—®é¢˜
        """
        if not sources:
            return []

        # å‡†å¤‡ä¿¡æºæ‘˜è¦
        sources_summary = []
        for i, s in enumerate(sources[:15]):  # æœ€å¤šåˆ†æ15ä¸ª
            sources_summary.append({
                "index": i,
                "domain": s.get("source_domain", ""),
                "title": s.get("title", "")[:80],
                "credibility": s.get("source_credibility", "medium"),
                "stance": s.get("source_stance", "neutral"),
                "insight": s.get("key_insight", "")[:100]
            })

        prompt = f"""ä½ æ˜¯ä¸€ä½ä¿¡æ¯åˆ†æä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹ä¿¡æºé›†åˆè¿›è¡Œæ·±åº¦åˆ†æã€‚

ã€åŸå§‹é—®é¢˜ã€‘
{original_content}

ã€æ ¸å¿ƒå®ä½“ã€‘
{', '.join(query_analysis.get('core_entities', []))}

ã€ä¿¡æºåˆ—è¡¨ã€‘
{json.dumps(sources_summary, ensure_ascii=False, indent=2)}

ã€ä½ çš„åˆ†æä»»åŠ¡ã€‘
1. è¯†åˆ«ä¿¡æºä¹‹é—´çš„å…±è¯†å’Œåˆ†æ­§
2. å‘ç°ä¿¡æ¯å†²çªç‚¹
3. è¯„ä¼°è¯æ®çš„å®Œæ•´æ€§
4. è¯†åˆ«å¯èƒ½çš„è°£è¨€ä¼ æ’­è·¯å¾„
5. æ‰¾å‡ºæœ€å…³é”®çš„çªç ´å£

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "source_analysis": [
        {{
            "index": 0,
            "analysis": "å¯¹è¯¥ä¿¡æºçš„æ·±åº¦åˆ†æ",
            "reliability_concerns": "å¯é æ€§æ–¹é¢çš„æ‹…å¿§ï¼ˆå¦‚æœ‰ï¼‰",
            "unique_value": "è¯¥ä¿¡æºçš„ç‹¬ç‰¹ä»·å€¼"
        }}
    ],
    "cross_source_patterns": "è·¨ä¿¡æºçš„æ¨¡å¼å‘ç°",
    "recommended_focus": [0, 2, 5]
}}

è¯·åˆ†æå‰10ä¸ªä¿¡æºï¼Œè¿”å›å®ƒä»¬çš„æ·±åº¦åˆ†æã€‚"""

        try:
            result_text = await self._call_llm_with_search(prompt)
            analysis_result = self._parse_search_result(result_text)
            
            # å°†åˆ†æç»“æœåˆå¹¶åˆ°åŸä¿¡æº
            source_analysis = analysis_result.get("source_analysis", [])
            for analysis in source_analysis:
                idx = analysis.get("index", 0)
                if idx < len(sources):
                    sources[idx]["deep_analysis"] = analysis.get("analysis", "")
                    sources[idx]["reliability_concerns"] = analysis.get("reliability_concerns", "")
                    sources[idx]["unique_value"] = analysis.get("unique_value", "")

            return sources
        except Exception as e:
            print(f"[SearchAgent] Deep analysis error: {e}")
            return sources

    async def _identify_key_findings(self, sources: List[Dict], original_content: str, query_analysis: Dict) -> Dict[str, Any]:
        """
        è¯†åˆ«å…³é”®å‘ç°ã€å†²çªç‚¹å’Œè¯æ®ç¼ºå£
        """
        if not sources:
            return {
                "findings": [],
                "conflict_points": [],
                "evidence_gaps": ["æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³ä¿¡æº"],
                "analysis_reasoning": "",
                "perspectives": {}
            }

        # å‡†å¤‡å…³é”®ä¿¡æ¯æ‘˜è¦
        key_info = []
        for s in sources[:12]:
            key_info.append({
                "domain": s.get("source_domain", ""),
                "insight": s.get("key_insight", "")[:150],
                "stance": s.get("source_stance", "neutral"),
                "credibility": s.get("source_credibility", "medium")
            })

        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº‹å®æ ¸æŸ¥ä¸“å®¶ã€‚åŸºäºæ”¶é›†åˆ°çš„ä¿¡æºï¼Œè¯·è¿›è¡Œæ·±å…¥åˆ†æã€‚

ã€å¾…æ ¸å®å†…å®¹ã€‘
{original_content}

ã€æ ¸å¿ƒé—®é¢˜ã€‘
{query_analysis.get('core_question', '')}

ã€æ”¶é›†åˆ°çš„å…³é”®ä¿¡æ¯ã€‘
{json.dumps(key_info, ensure_ascii=False, indent=2)}

ã€ä½ çš„åˆ†æä»»åŠ¡ã€‘
1. æç‚¼æ ¸å¿ƒå‘ç°ï¼ˆ3-5æ¡ï¼‰
2. è¯†åˆ«ä¿¡æ¯å†²çªç‚¹ï¼ˆä¸åŒä¿¡æºä¹‹é—´çš„çŸ›ç›¾ï¼‰
3. æŒ‡å‡ºè¯æ®ç¼ºå£ï¼ˆè¿˜éœ€è¦ä»€ä¹ˆä¿¡æ¯ï¼‰
4. åˆ†æä¸åŒç«‹åœºçš„è§‚ç‚¹
5. ç»™å‡ºä½ çš„ä¸“ä¸šåˆ¤æ–­å’Œæ¨ç†è¿‡ç¨‹

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "findings": [
        "æ ¸å¿ƒå‘ç°1ï¼šåŸºäºé«˜å¯ä¿¡åº¦ä¿¡æºçš„å…³é”®äº‹å®",
        "æ ¸å¿ƒå‘ç°2ï¼š..."
    ],
    "conflict_points": [
        "å†²çªç‚¹1ï¼šä¿¡æºAè¯´Xï¼Œä¿¡æºBè¯´Y",
        "å†²çªç‚¹2ï¼š..."
    ],
    "evidence_gaps": [
        "è¯æ®ç¼ºå£1ï¼šç¼ºå°‘å®˜æ–¹æ•°æ®",
        "è¯æ®ç¼ºå£2ï¼š..."
    ],
    "analysis_reasoning": "è¯¦ç»†çš„åˆ†ææ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä½ å¦‚ä½•æƒè¡¡ä¸åŒä¿¡æºã€å¦‚ä½•å¤„ç†å†²çªä¿¡æ¯",
    "perspectives": {{
        "supporting": "æ”¯æŒæ–¹çš„ä¸»è¦è§‚ç‚¹å’Œè¯æ®",
        "opposing": "åå¯¹æ–¹çš„ä¸»è¦è§‚ç‚¹å’Œè¯æ®",
        "neutral": "ä¸­ç«‹æ–¹çš„è§‚å¯Ÿ"
    }},
    "key_source_indices": [0, 3, 5]
}}

è¯·ç¡®ä¿åˆ†ææ·±å…¥ã€å®¢è§‚ã€ä¸“ä¸šã€‚"""

        try:
            result_text = await self._call_llm_with_search(prompt)
            return self._parse_search_result(result_text)
        except Exception as e:
            print(f"[SearchAgent] Key findings error: {e}")
            return {
                "findings": ["åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯"],
                "conflict_points": [],
                "evidence_gaps": [],
                "analysis_reasoning": f"é”™è¯¯: {str(e)}",
                "perspectives": {},
                "key_source_indices": []
            }

    async def _call_llm_with_search(self, prompt: str) -> str:
        """è°ƒç”¨æ”¯æŒè”ç½‘åŠŸèƒ½çš„ LLM (DeepSeek via é˜¿é‡Œç™¾ç‚¼)"""
        try:
            if self.llm_provider == "openai" and self.openai_client:
                print(f"[SearchAgent] Calling DeepSeek with web search...")
                
                # é˜¿é‡Œç™¾ç‚¼ DeepSeek è”ç½‘æœç´¢é…ç½®
                # å‚è€ƒ: https://help.aliyun.com/zh/model-studio/user-guide/deepseek
                response = await self.openai_client.chat.completions.create(
                    model=settings.OPENAI_MODEL,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿¡æ¯åˆ†æå¸ˆã€è°ƒæŸ¥è®°è€…å’Œäº‹å®æ ¸æŸ¥ä¸“å®¶ã€‚ä½ æ“…é•¿æ·±åº¦æœç´¢ã€æ‰¹åˆ¤æ€§æ€ç»´å’Œå¤šè§’åº¦åˆ†æã€‚ä½ æ€»æ˜¯åŸºäºè¯æ®è¯´è¯ï¼Œå–„äºå‘ç°ä¿¡æ¯å†²çªå’Œåè§ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.4,
                    max_tokens=4000,
                    # é˜¿é‡Œç™¾ç‚¼è”ç½‘æœç´¢é…ç½®
                    # ä½¿ç”¨ enable_search å‚æ•°å¯ç”¨è”ç½‘æœç´¢ï¼ˆé˜¿é‡Œç™¾ç‚¼ç‰¹å®šå‚æ•°ï¼‰
                    extra_body={
                        "enable_search": True
                    }
                )
                
                content = response.choices[0].message.content
                print(f"[SearchAgent] LLM Response: {content[:200]}...")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ç»“æœï¼ˆæœç´¢ç»“æœï¼‰
                if hasattr(response.choices[0], 'tool_calls') and response.choices[0].tool_calls:
                    print(f"[SearchAgent] Web search tool was used")
                
                return content
            elif self.llm_provider == "claude" and self.anthropic_client:
                # Claude ç›®å‰ä¸ç›´æ¥æ”¯æŒè”ç½‘æœç´¢ï¼Œéœ€è¦é…åˆå…¶ä»–æœç´¢å·¥å…·
                print(f"[SearchAgent] Claude does not support web search directly")
                response = self.anthropic_client.messages.create(
                    model=settings.ANTHROPIC_MODEL,
                    max_tokens=4000,
                    temperature=0.4,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text
            else:
                return "{}"
        except Exception as e:
            print(f"[SearchAgent] LLM Error: {str(e)}")
            return "{}"

    def _parse_search_result(self, result_text: str) -> Dict[str, Any]:
        """è§£æ LLM è¿”å›çš„æœç´¢ç»“æœ"""
        if not result_text or not result_text.strip():
            return {"sources": [], "search_reasoning": "", "findings": [], "conflict_points": [], "evidence_gaps": []}

        text = result_text.strip()

        # æ¸…ç† markdown
        if text.startswith("```json"):
            text = text[7:]
        elif text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        text = text.strip()

        try:
            result = json.loads(text)
            
            # ä¸ºæ¯ä¸ªä¿¡æºæ·»åŠ  ID å’Œé»˜è®¤å€¼
            if "sources" in result:
                for source in result["sources"]:
                    if not source.get("evidence_id"):
                        source["evidence_id"] = str(uuid.uuid4())
                    if not source.get("source_category"):
                        source["source_category"] = "news"
                    if not source.get("relevance_score"):
                        source["relevance_score"] = 0.8
                    if not source.get("evidence_type"):
                        source["evidence_type"] = "primary"
                    if not source.get("source_stance"):
                        source["source_stance"] = "neutral"
            
            return result
        except json.JSONDecodeError:
            # å°è¯•æå– JSON
            try:
                start = text.find("{")
                end = text.rfind("}") + 1
                if start >= 0 and end > start:
                    return json.loads(text[start:end])
            except:
                pass
            return {"sources": [], "search_reasoning": ""}

    def _deduplicate_sources(self, sources: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æŒ‰ URL å»é‡ï¼Œä¿ç•™æœ€å®Œæ•´çš„ç‰ˆæœ¬"""
        seen_urls = {}
        
        for source in sources:
            url = source.get("source_url", "")
            if url:
                if url not in seen_urls:
                    seen_urls[url] = source
                else:
                    # ä¿ç•™æ›´å®Œæ•´çš„ç‰ˆæœ¬ï¼ˆæœ‰æ›´å¤šå­—æ®µçš„ï¼‰
                    existing = seen_urls[url]
                    if len(str(source)) > len(str(existing)):
                        seen_urls[url] = source
        
        return list(seen_urls.values())

    def _rank_sources_by_importance(self, sources: List[Dict[str, Any]], key_findings: Dict) -> List[Dict[str, Any]]:
        """
        æŒ‰é‡è¦æ€§æ’åºä¿¡æºï¼Œå¹¶æ ‡è®°å…³é”®ä¿¡æº
        """
        key_indices = set(key_findings.get("key_source_indices", []))
        
        # ä¸ºæ¯ä¸ªä¿¡æºè®¡ç®—é‡è¦æ€§åˆ†æ•°
        for i, source in enumerate(sources):
            score = 0
            
            # å¯ä¿¡åº¦æƒé‡
            credibility_scores = {"high": 3, "medium": 2, "low": 1}
            score += credibility_scores.get(source.get("source_credibility", "low"), 1) * 10
            
            # ç›¸å…³åº¦æƒé‡
            score += source.get("relevance_score", 0.5) * 10
            
            # æ˜¯å¦è¢«æ ‡è®°ä¸ºå…³é”®ä¿¡æº
            if i in key_indices:
                score += 20
                source["is_key_source"] = True
            else:
                source["is_key_source"] = False
            
            # æ˜¯å¦æœ‰æ·±åº¦åˆ†æ
            if source.get("deep_analysis"):
                score += 5
            
            # æ˜¯å¦æœ‰ç‹¬ç‰¹ä»·å€¼
            if source.get("unique_value"):
                score += 3
            
            source["importance_score"] = score
        
        # æŒ‰é‡è¦æ€§åˆ†æ•°æ’åº
        return sorted(sources, key=lambda x: -x.get("importance_score", 0))
