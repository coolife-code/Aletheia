import json
import uuid
from typing import List, Dict, Any, AsyncGenerator
import openai
from anthropic import Anthropic
import hashlib

from app.core.config import settings


class ParserAgent:
    """æœç´¢ç­–ç•¥å¸ˆ Agent - æƒ…æŠ¥åˆ†æä¸æœç´¢æ–¹æ¡ˆè®¾è®¡"""

    _cache: Dict[str, Dict[str, Any]] = {}

    def __init__(self):
        self.llm_provider = settings.LLM_PROVIDER
        self.openai_client = openai.AsyncOpenAI(
            api_key=settings.OPENAI_API_KEY,
            base_url=settings.OPENAI_BASE_URL
        ) if settings.OPENAI_API_KEY else None
        self.anthropic_client = Anthropic(
            api_key=settings.ANTHROPIC_API_KEY
        ) if settings.ANTHROPIC_API_KEY else None

    def _get_cache_key(self, content: str) -> str:
        return hashlib.md5(content.encode()).hexdigest()

    async def parse(self, content: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢å¹¶ç”Ÿæˆæœç´¢æ–¹æ¡ˆ"""
        task_id = str(uuid.uuid4())

        cache_key = self._get_cache_key(content)
        if cache_key in self._cache:
            cached_result = self._cache[cache_key].copy()
            cached_result["task_id"] = task_id
            return cached_result

        analysis = await self._analyze_query(content)

        result = {
            "task_id": task_id,
            "original_query": content,
            "analysis": analysis,
            "search_queries": analysis.get("search_queries", []),
            "metadata": {
                "source_type": "user_input",
                "content_length": len(content),
                "timestamp": str(uuid.uuid1())
            }
        }

        self._cache[cache_key] = result.copy()
        return result

    async def parse_stream(self, content: str) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼åˆ†ææŸ¥è¯¢"""
        task_id = str(uuid.uuid4())

        cache_key = self._get_cache_key(content)
        if cache_key in self._cache:
            yield {
                "type": "reasoning",
                "agent": "parser",
                "step": "ç¼“å­˜å‘½ä¸­",
                "content": "å‘ç°ç¼“å­˜æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨å†å²åˆ†æç»“æœ"
            }
            cached_result = self._cache[cache_key].copy()
            cached_result["task_id"] = task_id
            yield {
                "type": "result",
                "agent": "parser",
                "data": cached_result
            }
            return

        # é—®é¢˜åˆ†æ - è¯¦ç»†æ¨ç†è¿‡ç¨‹
        yield {
            "type": "reasoning",
            "agent": "parser",
            "step": "é—®é¢˜åˆ†æ",
            "content": "ğŸ§  å¼€å§‹åˆ†æé—®é¢˜...\n"
                       "   1. æå–æ ¸å¿ƒå®ä½“ï¼šè¯†åˆ«äººç‰©ã€ç»„ç»‡ã€äº‹ä»¶ã€åœ°ç‚¹ç­‰å…³é”®è¦ç´ \n"
                       "   2. ç†è§£æŸ¥è¯¢æ„å›¾ï¼šåˆ¤æ–­ç”¨æˆ·æƒ³è¦éªŒè¯ä»€ä¹ˆä¿¡æ¯\n"
                       "   3. ç¡®å®šä¿¡æ¯ç±»å‹ï¼šäº‹å®éªŒè¯ / èƒŒæ™¯ä¿¡æ¯ / æ•°æ® / äººç‰©ä¼ è®°\n"
                       "   4. è¯„ä¼°éªŒè¯éœ€æ±‚ï¼šæ˜¯å¦éœ€è¦å¤šæºäº¤å‰éªŒè¯"
        }

        analysis = await self._analyze_query(content)

        # è¾“å‡ºè¯¦ç»†åˆ†æç»“æœ
        core_entities = analysis.get('core_entities', [])
        core_question = analysis.get('core_question', '')
        query_intent = analysis.get('query_intent', '')
        info_types = analysis.get('info_types', [])
        need_cross = analysis.get('need_cross_validation', False)

        yield {
            "type": "reasoning",
            "agent": "parser",
            "step": "å®ä½“è¯†åˆ«",
            "content": f"ğŸ” æ ¸å¿ƒå®ä½“è¯†åˆ«:\n" +
                       chr(10).join([f"   â€¢ {e}" for e in core_entities])
        }

        yield {
            "type": "reasoning",
            "agent": "parser",
            "step": "é—®é¢˜æç‚¼",
            "content": f"ğŸ¯ æ ¸å¿ƒé—®é¢˜:\n   {core_question}\n\n" +
                       f"ğŸ’­ æŸ¥è¯¢æ„å›¾:\n   {query_intent}"
        }

        yield {
            "type": "reasoning",
            "agent": "parser",
            "step": "ä¿¡æ¯ç±»å‹",
            "content": f"ğŸ“‹ ä¿¡æ¯ç±»å‹åˆ†æ:\n" +
                       chr(10).join([f"   â€¢ {t}" for t in info_types]) +
                       f"\n\nğŸ”— å¤šæºäº¤å‰éªŒè¯: {'éœ€è¦' if need_cross else 'ä¸éœ€è¦'}"
        }

        # æœç´¢ç­–ç•¥ - è¯¦ç»†è¯´æ˜
        search_strategy = analysis.get('search_strategy', '')
        yield {
            "type": "reasoning",
            "agent": "parser",
            "step": "æœç´¢ç­–ç•¥",
            "content": f"ğŸ“Š æœç´¢ç­–ç•¥è§„åˆ’:\n   {search_strategy}\n\n" +
                       "   ç­–ç•¥åŸåˆ™:\n" +
                       "   â€¢ ä»å®½æ³›åˆ°å…·ä½“ï¼šå…ˆäº†è§£æ•´ä½“èƒŒæ™¯ï¼Œå†æ·±å…¥ç»†èŠ‚\n" +
                       "   â€¢ å¤šè¯­è¨€è¦†ç›–ï¼šä¸­è‹±æ–‡ç»“åˆï¼Œè·å–æ›´å…¨é¢ä¿¡æ¯\n" +
                       "   â€¢ å¤šè§’åº¦éªŒè¯ï¼šå®˜æ–¹ã€åª’ä½“ã€ç¤¾äº¤å¹³å°å¤šæ–¹å°è¯\n" +
                       "   â€¢ æ—¶æ•ˆæ€§ä¼˜å…ˆï¼šä¼˜å…ˆè·å–æœ€æ–°ã€æœ€æƒå¨çš„ä¿¡æ¯"
        }

        # æœç´¢æŸ¥è¯¢ - é€ä¸ªè¯´æ˜
        queries = analysis.get("search_queries", [])
        yield {
            "type": "reasoning",
            "agent": "parser",
            "step": "æŸ¥è¯¢ç”Ÿæˆ",
            "content": f"ğŸ“ ç”Ÿæˆ {len(queries)} æ¡ç²¾å‡†æœç´¢æŸ¥è¯¢:"
        }

        for i, query in enumerate(queries, 1):
            yield {
                "type": "reasoning",
                "agent": "parser",
                "step": f"æŸ¥è¯¢{i}",
                "content": f"   {i}. {query}\n" +
                           f"      ç›®çš„: ä»ä¸åŒè§’åº¦æ”¶é›†ä¿¡æ¯ï¼Œç¡®ä¿å…¨é¢è¦†ç›–"
            }

        result = {
            "task_id": task_id,
            "original_query": content,
            "analysis": analysis,
            "search_queries": queries,
            "metadata": {
                "source_type": "user_input",
                "content_length": len(content),
                "timestamp": str(uuid.uuid1())
            }
        }

        self._cache[cache_key] = result.copy()
        yield {
            "type": "result",
            "agent": "parser",
            "data": result
        }

    async def _analyze_query(self, content: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢å¹¶ç”Ÿæˆæœç´¢æ–¹æ¡ˆ"""
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æƒ…æŠ¥åˆ†æå¸ˆå’Œæœç´¢ç­–ç•¥å¸ˆã€‚è¯·å¯¹ä»¥ä¸‹äº‹å®æ€§æŸ¥è¯¢è¿›è¡Œå®Œæ•´çš„æœç´¢å‰åˆ†æï¼Œå¹¶è®¾è®¡æœç´¢æ–¹æ¡ˆã€‚

æŸ¥è¯¢å†…å®¹ï¼š{content}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼ˆJSONæ ¼å¼ï¼‰ï¼š

{{
    "core_entities": ["æ ¸å¿ƒå®ä½“1", "æ ¸å¿ƒå®ä½“2", ...],
    "core_question": "æç‚¼åçš„æ ¸å¿ƒé—®é¢˜ï¼ˆä¸€å¥è¯ï¼‰",
    "query_intent": "æŸ¥è¯¢æ„å›¾è¯´æ˜",
    "info_types": ["äº‹å®éªŒè¯", "èƒŒæ™¯ä¿¡æ¯", "æ•°æ®", "äººç‰©ä¼ è®°"],
    "need_cross_validation": true/false,
    "search_strategy": "æœç´¢ç­–ç•¥è§„åˆ’ï¼ˆåŸåˆ™ï¼šä»å®½æ³›åˆ°å…·ä½“ã€å¤šè¯­è¨€ã€å¤šè§’åº¦ã€å¯éªŒè¯ï¼‰",
    "search_queries": [
        "æŸ¥è¯¢1ï¼ˆç²¾å‡†èšç„¦ï¼‰",
        "æŸ¥è¯¢2ï¼ˆç²¾å‡†èšç„¦ï¼‰",
        "æŸ¥è¯¢3ï¼ˆç²¾å‡†èšç„¦ï¼‰",
        ...
    ]
}}

è¦æ±‚ï¼š
1. æ ¸å¿ƒå®ä½“ï¼šæå–æŸ¥è¯¢ä¸­çš„å…³é”®äººç‰©ã€ç»„ç»‡ã€äº‹ä»¶ã€åœ°ç‚¹ç­‰
2. æ ¸å¿ƒé—®é¢˜ï¼šç”¨ä¸€å¥è¯ç²¾å‡†æ¦‚æ‹¬ç”¨æˆ·æƒ³é—®ä»€ä¹ˆ
3. æŸ¥è¯¢æ„å›¾ï¼šè¯´æ˜ç”¨æˆ·æƒ³è¦å¾—åˆ°ä»€ä¹ˆä¿¡æ¯
4. ä¿¡æ¯ç±»å‹ï¼šä» [äº‹å®éªŒè¯, èƒŒæ™¯ä¿¡æ¯, æ•°æ®, äººç‰©ä¼ è®°] ä¸­é€‰æ‹©
5. å¤šæºäº¤å‰éªŒè¯ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦å¤šä¸ªç‹¬ç«‹ä¿¡æºéªŒè¯
6. æœç´¢ç­–ç•¥ï¼šè¯´æ˜æœç´¢æ€è·¯ï¼Œå¦‚ä»å®½æ³›åˆ°å…·ä½“ã€å¤šè¯­è¨€ã€å¤šè§’åº¦ç­‰
7. æœç´¢æŸ¥è¯¢ï¼šç”Ÿæˆ3-6æ¡ç²¾å‡†æœç´¢æŸ¥è¯¢ï¼Œä¸­è‹±æ–‡éƒ½å¯ï¼Œæ¯æ¡èšç„¦ä¸åŒè§’åº¦

æ³¨æ„ï¼š
- æœç´¢æŸ¥è¯¢è¦å…·ä½“ã€å¯æ‰§è¡Œï¼ŒåŒ…å«å…³é”®å®ä½“
- æŸ¥è¯¢ä¹‹é—´è¦æœ‰å·®å¼‚åŒ–ï¼Œè¦†ç›–ä¸åŒè§’åº¦
- ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡æŸ¥è¯¢ï¼Œå¿…è¦æ—¶è¡¥å……è‹±æ–‡"""

        result_text = await self._call_llm(prompt)

        try:
            text = self._clean_json_text(result_text)
            result = json.loads(text)
            return result
        except Exception as e:
            print(f"[ParserAgent] Analysis error: {e}")
            # è¿”å›é»˜è®¤åˆ†æ
            return {
                "core_entities": [content[:20]],
                "core_question": content,
                "query_intent": "äº‹å®éªŒè¯",
                "info_types": ["äº‹å®éªŒè¯"],
                "need_cross_validation": True,
                "search_strategy": "ç›´æ¥æœç´¢æŸ¥è¯¢å†…å®¹",
                "search_queries": [content, f"{content} å®˜æ–¹é€šæŠ¥", f"{content} æœ€æ–°æ¶ˆæ¯"]
            }

    async def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨ LLM"""
        try:
            if self.llm_provider == "openai" and self.openai_client:
                response = await self.openai_client.chat.completions.create(
                    model=settings.OPENAI_MODEL,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æƒ…æŠ¥åˆ†æå¸ˆå’Œæœç´¢ç­–ç•¥å¸ˆï¼Œæ“…é•¿è®¾è®¡ç²¾å‡†çš„æœç´¢æ–¹æ¡ˆã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=2000
                )
                content = response.choices[0].message.content
                print(f"[ParserAgent] LLM Response: {content[:100]}...")
                return content
            elif self.llm_provider == "claude" and self.anthropic_client:
                response = self.anthropic_client.messages.create(
                    model=settings.ANTHROPIC_MODEL,
                    max_tokens=2000,
                    temperature=0.3,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                content = response.content[0].text
                print(f"[ParserAgent] LLM Response: {content[:100]}...")
                return content
            else:
                print(f"[ParserAgent] Warning: No LLM client available")
                return "{}"
        except Exception as e:
            print(f"[ParserAgent] LLM Error: {str(e)}")
            return "{}"

    def _clean_json_text(self, text: str) -> str:
        """æ¸…ç†JSONæ–‡æœ¬"""
        if not text:
            return "{}"

        text = text.strip()

        if text.startswith("```json"):
            text = text[7:]
        elif text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]

        return text.strip()
